{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://tau-data.id/umi/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img alt=\"\" src=\"images/0_Cover.jpg\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">Modul 07: Pendahuluan Social Media Analytics dan Natural Language Processing</font></center>\n",
    "<b><center>(C) Taufik Sutanto - 2019</center>\n",
    "<center>tau-data Indonesia ~ https://tau-data.id ~ taufik@tau-data.id</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color=\"blue\">Pendahuluan SMA & NLP: Data Gathering, text Preprocessing, & Text Analytics</font></center>\n",
    "<img alt=\"\" src=\"images/PDS_logo.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color=\"blue\">Workshop Schedule</font>\n",
    "\n",
    "## <font color=\"green\">Hari ke-4 (Kamis, 30 Jan 2020)</font>\n",
    "\n",
    "**Pendahuluan Natural Language Processing & Text PreProcessing**\n",
    "* 09:00 – 11:00 Crawling, Streaming, Scraping\n",
    "* 11:00 – 12:00\tText Preprocessing\n",
    "* 13:00 – 14:00\tText Analytics\n",
    "* 14:00 – 15.00\tSentiment Analysis\n",
    "* 15:00 – 16.00\tLatihan Text Analytics dan Sentiment Analysis \n",
    "\n",
    "Studi Kasus: **Text Analytics data media sosial perbincangan agama di media sosial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Modules for Google Colab\n",
    "!wget https://raw.githubusercontent.com/taufikedys/UMI/master/taudata.py\n",
    "!mkdir data\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/slang.dic\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/stopwords_id.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/stopwords_en.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/data.html\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UMI/master/data/kata_dasar.txt\n",
    "!pip install unidecode\n",
    "!pip install pyLDAvis\n",
    "!pip install textblob\n",
    "!pip install sastrawi\n",
    "!pip install twython\n",
    "!pip install tweepy\n",
    "!pip install spacy\n",
    "!pip install python-crfsuite\n",
    "!python -m spacy download en\n",
    "!python -m spacy download xx\n",
    "!python -m spacy download en_core_web_sm\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import taudata as tau, seaborn as sns; sns.set()\n",
    "import tweepy, json, nltk, urllib.request\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from twython import TwythonStreamer  \n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering: Crawling, Streaming, Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Analytics (SMA)\n",
    "\n",
    "<h3 id=\"SMA-adalah-sebuah-proses-pengumpulan-data-dari-media-sosial-dan-analisanya-untuk-mendapatkan-'insights'-atau-informasi-berharga-untuk-suatu-tujuan-tertentu-(definisi-adopted-dari-Gartner*)\">SMA adalah sebuah proses pengumpulan data dari media sosial dan analisanya untuk mendapatkan &#39;insights&#39; atau informasi berharga untuk suatu tujuan tertentu (definisi diadopsi dari Gartner*).</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/8_SMA.JPG\" style=\"width: 600px; height: 304px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/8_SMA_Cycle.JPG\" style=\"height:300px; width:705px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/8_SMA_Techniques.JPG\" style=\"height:400px; width:574px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Crawling-Data\">Crawling/Scrapping Data</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Digital_Media_Crawling_.png\" /></p>\n",
    "\n",
    "* Credits, image source: https://www.promptcloud.com/blog/scraping-social-media-data-for-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Social-Media-Analytics-Challenges\"><u>Tantangan Social Media Analytics</u></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>\n",
    "\t<p>Pendek (<strong>Short </strong>in lengths): bahkan terkadang tidak mengandung sebuah kalimat yang utuh menurut tata bahasa (grammar).</p>\n",
    "\t</li>\n",
    "\t<li><strong>Noise&nbsp;</strong>: Data media sosial penuh dengan noise seperti typos (salah ketik), encoding yang tidak jamak, slang, dsb.</li>\n",
    "\t<li><strong>Temporal&nbsp;</strong>: Informasi yang sedang trending biasanya hanya sesaat,<br />\n",
    "\tsehingga SMA diharapkan dilakukan dengan cepat menggunakan model-model/teknik-teknik analisa data yang efisien.</li>\n",
    "\t<li><strong>High-dimensional</strong> : Data di Media Sosial (Teks, Gambar, Video, Suara, dsb) adalah data tidak terstruktur berdimensi tinggi.</li>\n",
    "\t<li><strong>Fine-grained</strong> : Data di media sosial berasal dari banyak user yang masing-masingnya bisa jadi membahas beberapa topik yang berbeda.<br />\n",
    "\tSehingga komunitas (kelompok), topik, maupun klasifikasi yang ada menjadi besar (fine-grained).</li>\n",
    "\t<li><strong>Large in volume</strong>&nbsp;&amp; <strong>High velocity</strong>:&nbsp; Data yang sangat besar dan bertambah besar dengan cepat.</li>\n",
    "\t<li><strong>A lot of external Information</strong> : Informasi terkadang lebih banyak terkandung dari luar (eksternal) seperti url website, video, atau hal lain yang dibagikan oleh pengguna media sosial.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Case-Study:-twitter\">Case Study: twitter</h2>\n",
    "<ol>\n",
    "\t<li>API Keys</li>\n",
    "\t<li>Rules</li>\n",
    "\t<li>Crawling by searching</li>\n",
    "    <li>tweet Json</li>\n",
    "\t<li>Crawling by Streaming and Scrapping</li>\n",
    "    <li>twitter Social Media Analytics</li>\n",
    "</ol>\n",
    "<img alt=\"\" src=\"images/6_twitter.png\" style=\"width: 300px; height: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>twitter API Keys</h2>\n",
    "\n",
    "<h2><img alt=\"\" src=\"images/6_Creating_API_Keys.png\" style=\"width: 854px ; height: 444px\" /></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Aturan-twitter\">Aturan, bentuk data, &amp; error codes twitter</h2>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/rest/public/rate-limiting\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/rest/public/rate-limiting\" target=\"_blank\">dev.twitter.com/rest/public/rate-limiting</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/terms/agreement-and-policy\" target=\"_blank\">https://dev.twitter.com/overview/terms/agreement-and-policy</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/api/response-codes\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/overview/api/response-codes\" target=\"_blank\">dev.twitter.com/overview/api/response-codes</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/api/tweets\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/overview/api/tweets\" target=\"_blank\">dev.twitter.com/overview/api/tweets</a></p>\n",
    "\t</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh API Keys (Sesuaikan dengan API keys masing-masing)\n",
    "Ck = '' # consumer_key\n",
    "Cs = '' # consumer_secret\n",
    "At = '' # access_token\n",
    "As = '' # access_secret\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Koneksi ke twitter\n",
    "twitter = tau.twitter_connect(Ck, Cs, At, As)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'makassar'\n",
    "n = 10000 # jumlah tweet yang ingin diambil\n",
    "Tweets = tau.getTweets(twitter, topic, N = n, lan='id') # lan = 'id' atau 'en' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bentuk tweet = Json. Contoh tweet pertama:\n",
    "Tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengakses data spesifik pada tweet yang pertama:\n",
    "print('tweet pertama oleh \"{}\" : \"{}\"'.format(Tweets[0]['user']['screen_name'],Tweets[0]['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan hasil crawling twitter\n",
    "fileName = 'data/Tweets.json'\n",
    "tau.saveTweets(Tweets,file=fileName)\n",
    "print('Saved to '+fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me-load kembali jika (misal) analisa ingin dilakukan di lain waktu\n",
    "# Sengaja nama variabelnya saya bedakan (T2)\n",
    "T2 = tau.loadTweets(file='data/Tweets.json')\n",
    "print('tweet pertama oleh \"{}\" : \"{}\"'.format(T2[0]['user']['screen_name'],T2[0]['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengambil hanya data tweet\n",
    "D = [t['full_text'] for t in T2]\n",
    "D[:5] # 5 tweet pertama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Pemilihan-KeyWords\">Pemilihan KeyWords yang baik</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li><a href=\"https://medium.com/lingvo-masino/how-to-choose-keywords-in-twitter-9c3b85c50290\" target=\"_blank\">https://medium.com/lingvo-masino/how-to-choose-keywords-in-twitter-9c3b85c50290</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Operator (penting)\n",
    "\n",
    "<ul>\n",
    "\t<li><img alt=\"\" src=\"images/query_Operator.png\" style=\"width: 661px; height: 554px;\" /></li>\n",
    "    <li>Detail: <a href=\"https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators.html\" target=\"_blank\">https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators.html</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari kita coba #1\n",
    "twitter = tau.twitter_connect(Ck, Cs, At, As)\n",
    "topic = 'from:jokowi'\n",
    "n = 30 # Contoh jumlah tweet yang ingin diambil\n",
    "T = tau.getTweets(twitter, topic, N = n) # lan = 'id' atau 'en' ...\n",
    "[t['full_text'] for t in T]\n",
    "\n",
    "# Lesson ... becarefull with duplicates ==> use Hash Function!!!... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go to: https://www.latlong.net/convert-address-to-lat-long.html\n",
    "\n",
    "## Get Latitude dan Longitude dari alamat yang diinginkan\n",
    "\n",
    "**Catt**: Proses ini bisa juga digantikan dengan Google Maps API (gratis 40.000 query/bulan) dan bisa dijalankan langsung dalam program Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari kita coba #3 gunakan google (map) untuk koordinat suatu lokasi\n",
    "# http://thoughtfaucet.com/search-twitter-by-location/\n",
    "# misal search tweet tentang \"makanan\" di Depok dan sekitarnya\n",
    "\n",
    "auth = tweepy.auth.OAuthHandler(Ck, Cs)\n",
    "auth.set_access_token(At, As)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "Geo, N = \"-5.140370,119.480620,30km\", 30\n",
    "qry = '\"coto kuda\"'\n",
    "for tweet in tweepy.Cursor(api.search,q=qry,count=100,geocode=Geo).items(N):\n",
    "    print([tweet.created_at, tweet.text.encode('utf-8'), tweet.user.id, tweet.geo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Streaming-Data\">Streaming and Scrapping Data</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Meme_Streaming_Data.jpg\" style=\"width: 307px; height: 309px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming tweets. Untuk percobaan pilih topicS sesuatu yg sedang trending/populer \"saat ini\".\n",
    "# Atau bisa coba dengan mengirim tweet sendiri :)\n",
    "def streamTwitter(topicS, lang):\n",
    "    class MyStreamer(TwythonStreamer):\n",
    "        def on_success(self, data):\n",
    "            global count\n",
    "            count+=1\n",
    "            print('tweet from {}, post: {}'.format(data['user']['screen_name'], data['text']))\n",
    "            if count==maxTweet:\n",
    "                print('\\nFinished streaming %.0f tweets' %(maxTweet)); self.disconnect()\n",
    "        def on_error(self, status_code, data):\n",
    "            print('Error Status = %s' %status_code); self.disconnect()\n",
    "\n",
    "    while count<maxTweet:\n",
    "        stream = MyStreamer(Ck, Cs, At, As)\n",
    "        stream.statuses.filter(track=topicS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweet, count = 2, 0 # Rubah sesuai dengan kebutuhan, Untuk percobaan ini cukup (misal) 3 tweet\n",
    "lang = set(['en','id']) # bahasa bisa dipilih > 1\n",
    "topicS = ['coto duren'] # Bisa>1\n",
    "\n",
    "streamTwitter(topicS, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping (going beyond 7 days)\n",
    "\n",
    "### Solusi bagi yang belum memiliki API keys\n",
    "\n",
    "1. Buka browser **FireFox**\n",
    "2. Go to: https://twitter.com/search-advanced   (Tidak perlu login)\n",
    "3. Search sesuai keinginan/kebutuhan\n",
    "4. Save as Web Complete (misal di desktop) sebagai **data.html**\n",
    "5. Upload ke Google Colab\n",
    "6. Jalankan cell dibawah ini\n",
    "7. Unduh (download) file csv hasil scrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengubah html hasil scrapping ke CSV\n",
    "fileSource = 'data.html'\n",
    "fileOutput = 'data_twitter.csv'\n",
    "\n",
    "tau.twitter_html2csv(fileSource, fileOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "URL = 'http://bps.go.id'\n",
    "Doc = urllib.request.urlopen(URL).read()\n",
    "Doc = bs(Doc,'lxml').text\n",
    "print(Doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URL = 'http://bps.go.id'\n",
    "doc = urllib.request.urlopen(URL).read()\n",
    "doc = tau.text_from_html(doc)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) & Text PreProcessing\n",
    "\n",
    "<p><img alt=\"\" src=\"images/NLP.jpg\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline Bahasan NLP :\n",
    "* Pendahuluan\n",
    "* Tokenisasi\n",
    "* Stemming dan Lemma\n",
    "* Pos tag \n",
    "* WordNet dan WSD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing (NLP) - Pemrosesan Bahasa Alami (PBA):\n",
    "\n",
    "<p>\n",
    "&quot;<big><em>Sebuah cabang ilmu&nbsp;(AI/Computational Linguistik) yang mempelajari bagaimana&nbsp;bahasa (alami) manusia (terucap/tertulis) dapat dipahami dengan baik oleh komputer dan komputer dapat merespon dengan cara yang serupa ke manusia</em></big>&quot;.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><img alt=\"\" src=\"images/1_jarvis.jpg\" style=\"height: 450px; width: 600px;\" /></p>\n",
    "\n",
    "<p><a href=\"https://www.turn-on.de/lifestyle/topliste/zehn-film-gadgets-die-wir-uns-im-wahren-leben-wuenschen-4413\" target=\"_blank\"><strong>[Image Source]</strong></a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Aplikasi Umum NLP:</strong></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Machine Translation (Misal&nbsp;https://translate.google.com/ )</li>\n",
    "\t<li>Information Retrieval (IR)&nbsp;(misal www.google.com, bing, elasticsearch, etc.)</li>\n",
    "\t<li>Man-Machine Interface (misal Siri, cortana, atau Alexa)</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Apakah Perbedaan antara NLP dan Text Mining (TM)?</strong></p>\n",
    "\n",
    "<p>TM (terkadang disebut Text Analytics) adalah sebuah pemrosesan teks (biasanya dalam skala besar) untuk menghasilkan (generate) informasi atau insights. Untuk menghasilkan informasi TM menggunakan beberapa metode, termasuk NLP. TM mengolah teks secara eksplisit, sementara NLP mencoba mencari makna latent (tersembunyi) lewat aturan bahasa (e.g. grammar/idioms/Semantics).<br />\n",
    "<strong>Contoh aplikasi TM</strong> : Clustering, Klasifikasi, Social Media Analytics (SMA).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"https://www.kdnuggets.com/2017/11/framework-approaching-textual-data-tasks.html\" src=\"images/1_NLP_TextMining.jpg\" style=\"height: 470px; width: 600px;\" /></p>\n",
    "\n",
    "<p>[image source: <a href=\"https://www.elsevier.com/books/practical-text-mining-and-statistical-analysis-for-non-structured-text-data-applications/miner/978-0-12-386979-1\" target=\"_blank\">Gary M.:&quot;Practical Text Mining and Statistical Analysis for Non-structured Text Data Applications&quot;</a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/1_Text_Analytics.jpg\" style=\"height: 451px; width: 600px;\" /></p>\n",
    "\n",
    "<p>[Image Source: <a href=\"http://www.pearson.com.au/products/S-Z-Turban-Sharda/Business-Intelligence-and-Analytics-Systems-for-Decision-Support-Global-Edition/9781292009209?R=9781292009209\" target=\"_blank\">Efraim T. &quot;Business Intelligence and Analytics: Systems for Decision Support, Global Edition (10e)</a>&quot;]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi\n",
    "\n",
    "<p>Tokenisasi adalah pemisahan kata, simbol, frase, dan entitas penting lainnya (yang disebut sebagai token) dari sebuah teks untuk kemudian di analisa lebih lanjut. Token dalam NLP sering dimaknai dengan &quot;sebuah kata&quot;, walau tokenisasi juga bisa dilakukan ke kalimat, paragraf, atau entitas penting lainnya (misal suatu pola string DNA di Bioinformatika).</p>\n",
    "\n",
    "<p><strong>Mengapa perlu tokenisasi?</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Langkah penting dalam preprocessing, menghindari kompleksitas mengolah langsung pada string asal.</li>\n",
    "\t<li>Menghindari masalah (semantic) saat pemrosesan model-model natural language.</li>\n",
    "\t<li>Suatu tahapan sistematis dalam merubah unstructured (text) data ke bentuk terstruktur yang lebih mudah di olah.</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images\\2_Pipeline_Tokenization.png\" style=\"height:300px; width:768px\" /><br />\n",
    "[<a href=\"https://www.softwareadvice.com/resources/what-is-text-analytics/\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Tokenisasi-dengan-modul-NLTK\">Tokenisasi dengan modul NLTK</h2>\n",
    "\n",
    "<p><strong>Kelebihan</strong>:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Well established dengan dukungan bahasa yang beragam</li>\n",
    "\t<li>Salah satu modul NLP dengan fungsi terlengkap, termasuk WordNet</li>\n",
    "\t<li>Free dan mendapat banyak dukungan akademis.</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>&quot;Tidak support&quot;&nbsp;bahasa Indonesia</li>\n",
    "\t<li>Murni Python: relatif lebih lambat</li>\n",
    "</ol>\n",
    "\n",
    "<p><big><strong><a href=\"https://www.nltk.org/\" target=\"_blank\">https://www.nltk.org/</a></strong></big></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "Word_Tokens = nltk.word_tokenize(T)\n",
    "print(Word_Tokens) # tokenisasi kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Bandingkan jika menggunakan fungsi split di Python, apakah bedanya? \n",
    "print(T.split())\n",
    "# Apakah kesimpulan yang bisa kita tarik?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence_Tokens = nltk.sent_tokenize(T)\n",
    "print(Sentence_Tokens) # Tokenisasi kalimat\n",
    "# Perhatikan hasilnya, ada berapa kalimat yang di deteksi? setuju?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = \"how are you\\n are you okay?\"\n",
    "print(T)\n",
    "Sentence_Tokens = nltk.sent_tokenize(T)\n",
    "print(Sentence_Tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Diskusi:\"><font color=\"blue\">Diskusi:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Apakah tanda baca seperti &quot;?&quot; atau &quot;!&quot; akan memisahkan kalimat?</li>\n",
    "\t<li>Apakah tanda &quot;carriage return&quot;/enter/ganti baris memisahkan kalimat?</li>\n",
    "\t<li>Apakah &quot;;&quot; memisahkan kalimat?</li>\n",
    "\t<li>Apakah tanda dash &quot;-&quot; memisahkan kata? Dalam bahasa Indonesia/Inggris?</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Tips</strong>: Perhatikan bentuk <em>struktur data</em> &quot;output&quot; dari tokenisasi NLTK.<br />\n",
    "<strong>Catatan</strong>: pindah baris di Python string bisa dilakukan dengan menggunakan symbol &quot;\\n&quot;<br />\n",
    "<strong>Contoh</strong>:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi dengan modul <font color=\"blue\">Spacy</font>\n",
    "<strong>Kelebihan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Di claim lebih cepat (C-based)</li>\n",
    "\t<li>License termasuk untuk komersil</li>\n",
    "\t<li>Dukungan bahasa yang lebih banyak dari NLTK (termasuk bahasa Indonesia*)</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Fungsi yang lebih terbatas (dibandingkan NLTK).</li>\n",
    "\t<li>Karena berbasis compiler, sehingga instalasi cukup menantang.</li>\n",
    "</ol>\n",
    "\n",
    "<p><big><strong><a href=\"https://spacy.io/\" target=\"_blank\">https://spacy.io/</a></strong></big></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<p><img alt=\"\" src=\"images/2_Spacy.png\" style=\"height:434px; width:800px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Contoh tokenisasi menggunakan Spacy\n",
    "from spacy.lang.en import English\n",
    "nlp_en = English()\n",
    "\n",
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "nlp = nlp_en(T)\n",
    "for token in nlp:\n",
    "    print(token.text, end =', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en.add_pipe(nlp_en.create_pipe('sentencizer')) # New in latest Spacy\n",
    "nlp = nlp_en(T)\n",
    "for kalimat in nlp.sents:\n",
    "    print(kalimat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Diskusi:\"><font color=\"blue\">Diskusi:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Apakah hasil tokenisasi Spacy = NLTK? Mengapa?</li>\n",
    "</ul>\n",
    "\n",
    "<strong>Catatan</strong>: Contoh sederhana ini menekankan perbedaan ilmu linguistik dan computational linguistic.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenisasi dengan <font color=\"blue\"> TextBlob</font>\n",
    "<strong>Kelebihan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Sederhana &amp; mudah untuk digunakan/pelajari.</li>\n",
    "\t<li>Textblob objects punya behaviour/properties yang sama dengan string di Python.</li>\n",
    "\t<li>TextBlob dibangun dari kombinasi modul NLTK dan (Clips) Pattern</li>\n",
    "</ol>\n",
    "\n",
    "<p><strong>Kekurangan</strong>:</p>\n",
    "<ol>\n",
    "\t<li>Tidak secepat Spacy dan NLTK</li>\n",
    "\t<li>Language Model terbatas: English, German, French</li>\n",
    "</ol>\n",
    "\n",
    "<p>*Blob : Binary large Object</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizing di TextBlob\n",
    "T = \"Hello, Mr. Man. He smiled!! This, i.e. that, is it.\"\n",
    "print(TextBlob(T).words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimatS = TextBlob(T).sentences\n",
    "print([str(kalimat) for kalimat in kalimatS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Diskusi:\"><font color=\"blue\">Diskusi:</font></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Ada yang berbeda dari hasilnya?&nbsp;Apakah lebih baik seperti ini?</li>\n",
    "</ul>\n",
    "\n",
    "<p><strong>Tips</strong>: TextBlob biasa digunakan untuk prototyping pada data yang tidak terlalu besar.<br />\n",
    "<strong>Catatan</strong>: Hati-hati tipe data Blob tidak biasa (objek).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saat melakukan coding di Python, selalu perhatikan \"tipe data\" yang dihasilkan oleh modul.\n",
    "A = TextBlob(T).sentences\n",
    "B = TextBlob(T).words\n",
    "print(A[0], type(A[0]))\n",
    "print(B[0], type(B[0]))\n",
    "# Apakah bedanya dengan tipe data str biasa di python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = B[0] # <class 'textblob.blob.Word'>\n",
    "D = 'teks' # tipe string biasa di Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"properties\" Blob word\n",
    "print(dir(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"properties\" string di Python\n",
    "print(dir(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenisasi tidak hanya language dependent, tapi juga environment dependent\n",
    "\n",
    "<p>Tokenization sebenarnya tidak sesederhana memisahkan berdasarkan spasi dan removing symbol. Sebagai contoh dalam bahasa Jepang/Cina/Arab suatu kata bisa terdiri dari beberapa karakter.</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/2_Tokenization_Complexity.jpg\" style=\"height:500px; width:686px\" /><br />\n",
    "[<a href=\"http://aclweb.org/anthology/Y/Y11/Y11-1038.pdf\" target=\"_blank\"><strong>Image Source</strong></a>]</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Contoh Tokenizer untuk twitter\n",
    "Tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet = \"@Kirana_Sutanto I am so happpppppppy\"\n",
    "print(Tokenizer.tokenize(tweet))\n",
    "\n",
    "# Masih salah (i.e. \"happpy\"), nanti kita akan perbaiki ini dengan \"spell check\"\n",
    "# catatan: pada permasalahan \"Sentiment analysis\" kata yang ditulis panjang seperti diatas \n",
    "# bisa mengindikasikan sentiment yang kuat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Tokenisasi (NLP) Bahasa Indonesia:\n",
    "\n",
    "<p>NLTK belum support Bahasa Indonesia, bahkan module NLP Python yang support bahasa Indonesia secara umum masih sangat langka. Beberapa <u><strong>resources </strong></u>yang dapat digunakan:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li><strong><a href=\"https://github.com/kirralabs/indonesian-NLP-resources\" target=\"_blank\">KirraLabs</a></strong>: Mix of NLP-TextMining resources</li>\n",
    "\t<li><strong><a href=\"https://pypi.python.org/pypi/Sastrawi/1.0.1\" target=\"_blank\">Sastrawi 1.0.1</a>:</strong>&nbsp;untuk &quot;stemming&quot; &amp;&nbsp;<strong><a href=\"https://devtrik.com/python/stopword-removal-bahasa-indonesia-python-sastrawi/\" target=\"_blank\">stopwords&nbsp;</a></strong>bahasa Indonesia.</li>\n",
    "\t<li><strong><a href=\"http://stop-words-list-bahasa-indonesia.blogspot.co.id/2012/09/daftar-kata-dasar-bahasa-indonesia.html\" target=\"_blank\">Daftar Kata Dasar Indonesia</a></strong>:&nbsp;Bisa di load sebagai dictionary di Python</li>\n",
    "\t<li><strong><a href=\"https://id.wiktionary.org/wiki/Wiktionary:ProyekWiki_bahasa_Indonesia/Daftar_kata\" target=\"_blank\">Wiktionary</a></strong>: ProyekWiki bahasa Indonesia [termasuk Lexicon]</li>\n",
    "\t<li><a href=\"http://wn-msa.sourceforge.net/\" target=\"_blank\"><strong>WordNet Bahasa Indonesia</strong></a>: Bisa di load&nbsp;sebagai dictionary (atau NLTK<em>*</em>) di Python.</li>\n",
    "\t<li><strong><a href=\"http://kakakpintar.com/daftar-kata-baku-dan-tidak-baku-a-z-dalam-bahasa-indonesia/\" target=\"_blank\">Daftar Kata Baku-Tidak Baku</a></strong>: Bisa di load sebagai dictionary di Python.</li>\n",
    "\t<li><strong><a href=\"https://spacy.io/\" target=\"_blank\">Spacy</a></strong>: Cepat/efisien, MIT License, tapi language model Indonesia masih terbatas.</li>\n",
    "\t<li><a href=\"http://ufal.mff.cuni.cz/udpipe\" target=\"_blank\"><strong>UdPipe</strong></a>: Online request &amp; restricted license (support berbagai bahasa -&nbsp;pemrograman).</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Contoh Tokenisasi dalam bahasa Indonesia dengan Spacy\n",
    "from spacy.lang.id import Indonesian\n",
    "nlp_id = Indonesian()  # Language Model\n",
    "\n",
    "teks = 'Sore itu Hamzah melihat kupu-kupu di taman. Ibu membeli oleh-oleh di pasar'\n",
    "tokenS_id = nlp_id(teks)\n",
    "\n",
    "print([t for t in tokenS_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jika menggunakan Language model English:\n",
    "tokenS_en = nlp_en(teks)\n",
    "print([token.text for token in tokenS_en])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><u><big><strong>Word Case</strong></big></u><big> (Huruf BESAR/kecil):</big></p>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menganalisa makna (<em>semantic</em>) dari suatu (frase) kata dan mencari informasi dalam proses textmining, seringnya (*) kita tidak membutuhkan informasi huruf besar/kecil dari kata&nbsp;tersebut.</li>\n",
    "\t<li><em>Text case normaliation</em> dapat dilakukan pada string secara efisien tanpa melalui tokenisasi (mengapa?).</li>\n",
    "\t<li>Namun, bergantung pada analisa teks yang akan digunakan pengguna harus berhati-hati dengan urutan proses (pipelining) dalam preprocessing. Mengapa dan apa contohnya?</li>\n",
    "</ul>\n",
    "\n",
    "<p>(*) Coba temukan minimal 2 pengecualian dimana&nbsp; huruf kapital/kecil (case) mempengaruhi makna/pemrosesan teks.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Ignore case (huruf besar/kecil)\n",
    "T = \"Hi there!, I am a student. Nice to meet you :)\"\n",
    "print(T.lower())\n",
    "print(T.upper())\n",
    "# Perintah ini sangat efisien karena hanya merubah satu bit di setiap (awal) bytes dari setiap karakter\n",
    "# Sehingga tetap efisien jika ingin dilakukan sebelum tokenisasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Morphological-Linguistic Normalization: Stemming &amp; Lemmatization\n",
    "(Canonical Representation)\n",
    "<p><img alt=\"\" src=\"images/2_yoda.jpg\" style=\"height:400px; width:400px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\">Stemming dan Lemma</font>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Stemmer</strong>&nbsp;akan menghasilkan sebuah bentuk kata yang disepakati oleh suatu sistem tanpa mengindahkan konteks kalimat. Syaratnya beberapa kata dengan makna serupa hanya perlu dipetakan secara konsisten ke sebuah kata baku.&nbsp;Banyak digunakan di IR &amp;&nbsp;komputasinya relatif sedikit. Biasanya dilakukan dengan menghilangkan imbuhan (suffix/prefix).</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>lemmatisation</strong> akan menghasilkan kata baku (dictionary word) dan bergantung konteks.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p>Lemma &amp; stemming bisa jadi sama-sama menghasilkan suatu akar kata (root word). Misal : <em>Melompat </em>==&gt; <em>lompat</em></p>\n",
    "\t</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Mengapa melakukan Stemming &amp; Lemmatisasi</strong>?</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Sering digunakan di IR (Information Retrieval) agar ketika seseorang mencari kata tertentu, maka seluruh kata yang terkait juga diikutsertakan.<br />\n",
    "\tMisal:&nbsp;<em>organize</em>,&nbsp;<em>organizes</em>, and&nbsp;<em>organizing&nbsp;</em>&nbsp;dan&nbsp;<em>democracy</em>,&nbsp;<em>democratic</em>, and&nbsp;<em>democratization</em>.</li>\n",
    "\t<li>Di Text Mining Stemming dan Lemmatisasi akan mengurangi dimensi (mengurangi variasi morphologi), yang terkadang akan meningkatkan akurasi.</li>\n",
    "\t<li>Tapi di IR efeknya malah berkebalikan: <strong><font color=\"blue\">meningkatkan recall, tapi menurunkan akurasi&nbsp;</font></strong>[<a href=\"https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\" target=\"_blank\"><strong>Link</strong></a>]. Contoh: kata&nbsp;<em>operate, operating, operates, operation, operative, operatives, dan operational</em>&nbsp;jika di stem menjadi <em>operate</em>, maka ketika seseorang mencari &quot;<em>operating system</em>&quot;, maka entry seperti&nbsp;<em>operational and research</em> dan&nbsp;<em>operative and dentistry</em>&nbsp;akan muncul sebagai entry dengan relevansi yang cukup tinggi.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p><strong>Stemming tidak perlu &quot;benar&quot;, hanya perlu konsisten. Sehingga memiliki berbagai variansi, (sebagian) contoh di NLTK:</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Contoh Stemming di NLTK\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "T = 'presumably I would like to MultiPly my provision, saying tHat without crYing'\n",
    "print('Sentence: ',T)\n",
    "\n",
    "StemmerS = [LancasterStemmer, PorterStemmer, SnowballStemmer] \n",
    "Names = ['Lancaster', 'Porter', 'SnowBall']\n",
    "\n",
    "for stemmer_name,stem in zip(Names,StemmerS):\n",
    "    if stemmer_name == 'SnowBall':\n",
    "        st = stem('english')\n",
    "    else:\n",
    "        st = stem()\n",
    "        \n",
    "    print(stemmer_name,': ',' '.join(st.stem(t) for t in T.split()))\n",
    "# perhatikan, kita tidak melakukan case normalization (lowercase) \n",
    "# Hasil stemming bisa tidak bermakna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh Lemmatizer di NLTK\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "T = \"Apples and oranges are similar. boots and hippos aren't, don't you think?\"\n",
    "print('Sentence: ', T)\n",
    "print('Lemmatize: ',' '.join(lemmatizer.lemmatize(t) for t in T.split()))\n",
    "# Lemma case sensitive. Dengan kata lain string harus diubah ke dalam bentuk huruf kecil (lower case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer menggunakan informasi pos. \"pos\" (part-of-speech) akan dibahas di segmen berikutnya\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # adjective\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"v\")) # verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob Stemming & Lemmatizer\n",
    "from textblob import Word\n",
    "# Stemming\n",
    "print(Word('running').stem()) # menggunakan NLTK Porter stemmer\n",
    "\n",
    "# Lemmatizer\n",
    "print(Word('went').lemmatize('v'))\n",
    "\n",
    "# default Noun, plural akan menjadi singular dari akar katanya\n",
    "# Juga case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy Lemmatizer English\n",
    "from spacy.lang.en import English\n",
    "nlp_en = English()\n",
    "\n",
    "E = \"I am sure apples and oranges are similar. Boots and hippos aren't, don't you think?\"\n",
    "e = nlp_en(E)\n",
    "print( ' '.join( k.lemma_ for k in e ) )\n",
    "\n",
    "# Perhatikan huruf besar/kecil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy Lemmatizer Indonesia\n",
    "I = \"perayaan itu mengurus dengan saat kita menonton ke Jogjakarta\"\n",
    "idn = nlp_id(I)\n",
    "print( ' '.join( k.lemma_ for k in idn ) )\n",
    "# Hati-hati huruf besar/kecil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy \"tidak\" (bukan belum) support Stemming:\n",
    "\n",
    "<p><strong><img alt=\"\" src=\"images/2_Spacy_Pipelines.jpg\" style=\"height:400px; width:487px\" /></strong></p>\n",
    "\n",
    "<p>[<a href=\"https://spacy.io/usage/spacy-101#lightning-tour\" target=\"_blank\"><strong>Image Source</strong></a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer dengan Sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "stemmer = StemmerFactory().create_stemmer()\n",
    "\n",
    "I = \"perayaan itu Berbarengan dengan saat kita bepergian ke Makassar\"\n",
    "print(stemmer.stem(I))\n",
    "print(stemmer.stem(\"Perayaan Bepergian Menyuarakan\"))\n",
    "# Ada beberapa hal yang berbeda antara Sastrawi dan modul-modul diatas.\n",
    "# Apa sajakah?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h3 id=\"Tips:\">Tips:</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Secara umum &#39;biasanya&#39; di Text Mining yang kita butuhkan hanyalah <strong><font color=\"blue\">Lemma</font></strong>.</li>\n",
    "\t<li>&quot;Kecuali&quot; di aplikasi IR, spelling correction, variasi kata, clustering, atau terkadang klasifikasi. Pada aplikasi-aplikasi tersebut stemming terkadang lebih diinginkan.</li>\n",
    "\t<li>Stemming jauh lebih cepat, tapi tidak selalu tersedia di modul NLP.</li>\n",
    "\t<li>Beberapa algoritma tertentu membutuhkan tanda &quot;.&quot; dan &quot;,&quot; : contohnya untuk document summarization di textRank.</li>\n",
    "\t<li>&quot;_&quot; juga biasa digunakan untuk menyatakan frase kata di representasi n-grams (misal &quot;buah_tangan&quot;).</li>\n",
    "\t<li>Stemming juga digunakan pada Word Sense Disambiguation (WSD)</li>\n",
    "</ul>\n",
    "\n",
    "<h3 id=\"Diskusi:\">Diskusi:</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Untuk menghemat storage database, apakah sebaiknya kita menyimpan saja hasil preprocessed texts/documents?</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-of-Speech (pos) di ilmu bahasa (Linguistik)\n",
    "<p><img alt=\"\" src=\"images/2_parts-of-speech-chart.jpg\" style=\"height:400px; width:404px\" /></p>\n",
    "<p>[<a href=\"https://www.paperrater.com/page/parts-of-speech\" target=\"_blank\">image source</a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tags in NLTK (English)\n",
    "import nltk\n",
    "T = 'I am currently learning NLP in English, but if possible I want to know NLP in Indonesian language too'\n",
    "\n",
    "nltk_tokens = nltk.word_tokenize(T)\n",
    "print(nltk.pos_tag(nltk_tokens))\n",
    "# Tidak lagi hanya 9 macam tags seperti yang dibahas ahli bahasa (linguist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = nltk.pos_tag(nltk_tokens)\n",
    "Z[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering berdasarkan \"pos\"\n",
    "pos = ['NN','JJ']\n",
    "[h[0] for h in Z if h[1] in pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daftar NLTK pos-tags:\n",
    "<img alt=\"\" src=\"images/2_post_tags_NLTK.png\" style=\"height:400px; width:516px\" /></h3>\n",
    "\n",
    "<p>[<a href=\"http://gitqwerty777.github.io/natural-language-processing/\" target=\"_blank\">image source</a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos tags in TextBlob (English)\n",
    "for word, pos in TextBlob(T).tags:\n",
    "    print(word, pos, end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos Tag Spacy English\n",
    "import spacy\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "tokens = nlp_en(T)\n",
    "for tok in tokens:\n",
    "    #print(tok, tok.tag_, tok.lemma_, end = ', ')\n",
    "    print(tok, tok.tag_, end = ', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy tidak perlu tabel pos tag ...  bisa pakai perintah \"explain\"\n",
    "spacy.explain('RB')\n",
    "# Daftar Lengkap: https://spacy.io/api/annotation#pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos Tags in Spacy - Bahasa Indonesia?\n",
    "Ti = \"Saat bepergian ke Jogjakarta jangan lupa membeli oleh-oleh\"\n",
    "Teks = nlp_id(Ti)\n",
    "for token in Teks:\n",
    "    print(token.lemma_, token.tag_)\n",
    "# Fungsi pos-tags belum tersedia untuk bahasa indonesia .. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pos Tag Bahasa Indonesia lewat NLTK\n",
    "# https://yudiwbs.wordpress.com/2018/02/20/pos-tagger-bahasa-indonesia-dengan-pytho/\n",
    "from nltk.tag import CRFTagger\n",
    "ct = CRFTagger()\n",
    "ct.set_model_file('data/all_indo_man_tag_corpus_model.crf.tagger')\n",
    "\n",
    "hasil = ct.tag_sents([['Saya','bekerja','di','Bandung']])\n",
    "hasil = hasil[0]\n",
    "print(hasil)\n",
    "# Hati-hati dengan struktur data inputnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasil[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 id=\"Outline-Module-I.3.-:\">Outline PreProcessing Data Text :</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Filtering (stopwords)</li>\n",
    "\t<li>Replace slang/typos/singkatan</li>\n",
    "    <li>Spell Check</li>\n",
    "\t<li>Machine translation</li>\n",
    "\t<li>Reguler expression</li>\n",
    "\t<li>Encodings</li>\n",
    "    <li>Latihan</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Level Normalization: StopWords\n",
    "<p><u>Di Text Mining</u> kata-kata yang <strong>sering muncul </strong>(dan jarang sekali muncul) memiliki sedikit sekali informasi (signifikansi) terhadap model (machine learning) yang digunakan. Hal ini di karenakan kata-kata tersebut muncul di semua kategori (di permasalahan klasifikasi) atau di semua cluster (di permasalahan pengelompokan/clustering). Kata-kata yang sering muncul ini biasa disebut &quot;StopWords&quot;. Stopwords berbeda-beda bergantung dari Bahasa dan Environment (aplikasi)-nya.<br />\n",
    "<strong>Contoh</strong>:<br />\n",
    "\n",
    "<ul>\n",
    "\t<li>Stopwords bahasa Inggris: am, is, are, do, the, of, etc.</li>\n",
    "\t<li>Stopwords bahasa Indonesia: adalah, dengan, yang, di, ke, dsb</li>\n",
    "\t<li>Stopwords twitter: RT, ...<br />\n",
    "\t<img alt=\"\" src=\"images/2_StopWords.png\" style=\"height:250px; width:419px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Stopwords: Ada beberapa cara\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "factory = StopWordRemoverFactory()\n",
    "\n",
    "NLTK_StopWords = stopwords.words('english')\n",
    "Sastrawi_StopWords_id = factory.get_stop_words()\n",
    "Personal_StopWords_en = [t.strip() for t in tau.LoadDocuments(file = 'data/stopwords_en.txt')[0]]\n",
    "Personal_StopWords_id = [t.strip() for t in tau.LoadDocuments(file = 'data/stopwords_id.txt')[0]]\n",
    "\n",
    "print(NLTK_StopWords[:5])\n",
    "print(Sastrawi_StopWords_id[:5])\n",
    "print(Personal_StopWords_en[:5])\n",
    "print(Personal_StopWords_id[:5])\n",
    "print(len(Sastrawi_StopWords_id), len(Personal_StopWords_id), len(NLTK_StopWords), len(Personal_StopWords_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diskusi: Apakah sebaiknya kita menggunakan daftar stopwords bawaan modul atau custom milik kita sendiri?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipe variabel memiliki aplikasi optimal yang berbeda-beda, misal\n",
    "L = list(range(10**7))\n",
    "S = set(range(10**7)) # selain unik dan tidak memiliki keterurutan, set memiliki fungsi lain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "9000000 in L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "9000000 in S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips: selalu rubah list stopwords ke bentuk set, karena di Python jauh lebih cepat untuk cek existence di set ketimbang list\n",
    "NLTK_StopWords = set(NLTK_StopWords)\n",
    "Sastrawi_StopWords_id = set(Sastrawi_StopWords_id)\n",
    "Personal_StopWords_en = set(Personal_StopWords_en)\n",
    "Personal_StopWords_id = set(Personal_StopWords_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "St = ['ada', 'pesawat']\n",
    "'Ada' in St"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cara menggunakan stopwords\n",
    "from textblob import TextBlob\n",
    "\n",
    "T = \"I am doing NLP at UMI,... \\\n",
    "    adapun saya anu sedang belajar NLP di UMI\"\n",
    "T = T.lower()\n",
    "Personal_StopWords_id.add('anu')\n",
    "Tokens = TextBlob(T).words # Tokenisasi \n",
    "\n",
    "T1 = [t for t in Tokens if t not in Personal_StopWords_id] # Sastrawi_StopWords_id Personal_StopWords_en Personal_StopWords_id\n",
    "T2 = [t for t in T1 if t not in Personal_StopWords_en] # Sastrawi_StopWords_id Personal_StopWords_en Personal_StopWords_id\n",
    "print(' '.join(T1))\n",
    "print(' '.join(T2))\n",
    "# Catatan: Selalu lakukan Stopword filtering setelah tokenisasi (dan normalisasi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/2_Tokenization_Stopwords.png\" style=\"height:400px; width:765px\" /></p>\n",
    "\n",
    "<p>[<a href=\"http://chdoig.github.io/acm-sigkdd-topic-modeling/#/6/2\" target=\"_blank\">image source</a>]</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menangani Slang atau Singkatan di Data Teks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebuah contoh sederhana \n",
    "T = 'jangan ragu gan, langsung saja di order pajangannya.'\n",
    "# Misal kita hendak mengganti setiap singkatan (slang) dengan bentuk penuhnya. \n",
    "# Dalam hal ini kita hendak mengganti 'gan' dengan 'juragan'\n",
    "H = T.replace('gan','juragan')\n",
    "print(H)\n",
    "# Kita tidak bisa melakukan ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = {'yg':'yang', 'gan':'juragan'}\n",
    "D['gan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dengan tokenisasi\n",
    "slangs = {'gan':'juragan', 'yg':'yang', 'dgn':'dengan'} #dictionary sederhana berisi daftar singkatan dan kepanjangannya\n",
    "\n",
    "T = 'jangan ragu gan, langsung saja di order pajangan yg diatas.'\n",
    "T = TextBlob(T).words\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,t in enumerate(T):\n",
    "    if t in slangs.keys():\n",
    "        T[i] = slangs[t]\n",
    "print(' '.join(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Slang dan Singkatan dari File\n",
    "# Contoh memuat word fix melalui import file. \n",
    "slangS = tau.LoadDocuments(file = 'data/slang.dic')[0]\n",
    "slangS[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangS = [t.strip('\\n').strip() for t in slangS]\n",
    "print(slangS[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pisahkan berdasarkan ':'\n",
    "slangS = [t.split(\":\") for t in slangS]\n",
    "slangS = [[k.strip(), v.strip()] for k,v in slangS]\n",
    "print(slangS[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rubah list menjadi Dictionary\n",
    "slangS = {k:v for k,v in slangS}\n",
    "slangS['otw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slangS['tsb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test it!\n",
    "tweet = 'I luv  and say. serius gan!, tapi ndak tau kalau sesok.'\n",
    "T = TextBlob(tweet).words\n",
    "\n",
    "for i,t in enumerate(T):\n",
    "    if t in slangS.keys():\n",
    "        T[i] = slangS[t]\n",
    "        \n",
    "print(' '.join(T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Spell Check:</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/2_SpellCheck.jpg\" style=\"height:414px; width:499px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tujuan Spellcheck:</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>\tCleaning Data\t</li>\n",
    "\t<li>Word suggestions</li>\n",
    "\t<li>OCR/hand writing (Image) recognition</li>\n",
    "\t<li>Speech Recognition</li>\n",
    "\t<li>Machine Translation</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aplikasi spell check di textBlob\n",
    "from textblob import Word\n",
    "\n",
    "w = Word('applr')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Word('jaggan')\n",
    "w.spellcheck()\n",
    "# Kendalanya kalau Bahasa Indonesia ==> perlu pendekatan umum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Norvig Spell Checker: Menggunakan Aturan Probabilitas Bayes&nbsp;</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/2_Bayes_Norig.JPG\" style=\"height:500px; width:919px\" /></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://norvig.com/spell-correct.html\n",
    "# corpus = 'data/kata_dasar.txt'\n",
    "print(tau.correction('indurfu'))\n",
    "print(tau.correction('yanh'))\n",
    "# prinsip yang sama kelak bisa diterapkan untuk n-grams dengan merubah karakter ==> kata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Language Detection and Translation\n",
    "<p><img alt=\"\" src=\"images/2_translation.jpg\" style=\"height:400px; width:534px\" /></p>\n",
    "\n",
    "\n",
    "<p>[<a href=\"https://www.slideshare.net/PhuLeTruongVinh/ltvp-thesis-defense\" target=\"_blank\">image Source</a>]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Language Detection (TextBlob)\n",
    "from textblob import TextBlob\n",
    "T = \"Aku ingin mengerti NLP dalam bahasa Inggris\"\n",
    "U = \"opo iso mangan sa iki\"\n",
    "print(TextBlob(T).detect_language())\n",
    "print(TextBlob(U).detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Translation (TextBlob)\n",
    "# Butuh koneksi internet, limited calls. Error otherwise. Need \"try\" and \"catch\".\n",
    "T = \"Aku ingin mengerti NLP dalam bahasa Inggris. I love you\"\n",
    "print(TextBlob(T).translate(to='en'))\n",
    "print(TextBlob(T).translate(to='ar-sa'))\n",
    "print(TextBlob(T).translate(to='ja'))\n",
    "# daftar kode bahasa : http://www.cardinalpath.com/resources/tools/google-analytics-language-codes/\n",
    "# Perhatikan TextBlob secara automatis akan mendeteksi bahasa asal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kalau secara spesifik ingin translate dari suatu bahasa ke bahasa lain:\n",
    "T = \"Aku hanya ingin mengatakan aku lelah - super alay status\"\n",
    "print(TextBlob(T).translate(from_lang ='id', to='en'))\n",
    "print(TextBlob(T).translate(from_lang ='en', to='id'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/2_regex_meme.jpg\" style=\"height:397px; width:599px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Beberapa Reguler Expression yang sering digunakan di NLP/Text Mining</strong></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Menghilangkan/extract email</li>\n",
    "\t<li>Menghilangkan/extract nomer telephone</li>\n",
    "\t<li>Menghilangkan/extract URL di string.</li>\n",
    "\t<li>Alpha Numeric filtering</li>\n",
    "\t<li>Wild Card Search</li>\n",
    "\t<li>Cleaning hashTags di Media Sosial</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting atau replacing eMail.\n",
    "import re\n",
    "emailPattern = re.compile(r'[\\w._%+-]+@[\\w\\.-]+\\.[a-zA-Z]{2,4}')\n",
    "\n",
    "txt = 'Contact kami di admin@nlpindonesia.org, nlp.indonesia@sci.yahoo.co.id, atau nlp_nusantara@internet.net'\n",
    "\n",
    "print( re.sub(emailPattern, ' ', txt) )# clean email\n",
    "eMailS = re.findall( emailPattern, txt )\n",
    "print( 'email yang ditemukan: ', str(eMailS) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/2_email_regex.gif\" style=\"height:403px; width:687px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pola telephone : \\d penanda angka di reguler Expression, \\s spasi, dan \"|\" adalah \"atau\"\n",
    "# \"?\" menyatakan pilihan (optional): colou?r sesuai dengan colour atau color.\n",
    "\n",
    "phonePattern = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "txt = 'Contact kami di 021-7634562 atau 021-763-4562 atau 021 763 4562 atau 0822959020 atau +628199258688'\n",
    "print(re.sub(phonePattern,'***',txt))# clean phone\n",
    "    \n",
    "noTelp = re.findall(phonePattern,txt)\n",
    "print('Nomer telephone yang ditemukan: ',str(noTelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pola telephone 2: untuk setiap angka 8-14 digits dipisahkan oleh \"spasi\", \",\" atau \".\"\n",
    "phonePattern = re.compile(r'\\b\\d{8,14}\\b')\n",
    "txt = 'Contact kami di 082295203040 atau +6282295203040'\n",
    "print(re.sub(phonePattern,'***',txt))# clean phone\n",
    "    \n",
    "noTelp = re.findall(phonePattern,txt)\n",
    "\n",
    "for no in noTelp:\n",
    "    if no[0]!='0':\n",
    "        print('+' + no)\n",
    "    else:\n",
    "        print(no)\n",
    "#print('Nomer telephone yang ditemukan: ',str(noTelp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Website URLS http(s) .... untuk ftp trivial\n",
    "urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "txt = 'website reguler expression & my site : https://www.regular-expressions.info/ & https://tau-data.id'\n",
    "print(re.sub(urlPattern,' ',txt))# clean urls\n",
    "\n",
    "URLs = re.findall(urlPattern,txt)# get URLs\n",
    "print('URL yang ditemukan: ',str(URLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning non alpha-numeric\n",
    "txt = 'Hi! @Mukidi, apa kabar? #sapa_Pagi.'\n",
    "print(re.sub(r'[^\\w]',' ',txt))\n",
    "# atau jika ingin exclude titik dan koma \n",
    "# re.sub(r'[^.,a-zA-Z0-9 \\n\\.]','',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative 2:\n",
    "print(''.join([t for t in txt if t.isalnum() or t==' ']))\n",
    "# ada perbedaan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"\" src=\"images/2_nonStandard_Language.jpg\" style=\"height:359px; width:638px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning hashTags dalam posting media sosial\n",
    "tweet = 'oh IoT, #AndaiSajaIaTahu #ApaYangAkuRasah... #AlayersTweet #d2d'\n",
    "\n",
    "getHashtags = re.compile(r\"#(\\w+)\")\n",
    "print(\"Tags = {0}\".format(re.findall(getHashtags, tweet)))\n",
    "# temukan hanya tags ... perhatikan IoT bukan Tags walau ada huruf besar & kecil dalam satu kata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pisahtags = re.compile(r'[A-Z][^A-Z]*')\n",
    "\n",
    "for tags in re.findall(getHashtags, tweet):\n",
    "    print(re.findall(pisahtags, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengganti hashtags dengan kata dasar pembentuknya\n",
    "tweet = 'oh IoT, #AndaiSajaIaTahu #ApaYangAkuRasah... #AlayersTweet'\n",
    "tagS = re.findall(getHashtags, tweet)\n",
    "for tag in tagS:\n",
    "    proper_words = ' '.join(re.findall(pisahtags, tag))\n",
    "    tweet = tweet.replace('#'+tag,proper_words)\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'The25XYZ3abc'\n",
    "re.split('(\\d+)',s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding-Decoding:\n",
    "\n",
    "<ul>\n",
    "\t<li>Hal berikutnya yang perlu diperhatikan dalam memproses data teks adalah encoding-decoding.</li>\n",
    "\t<li>Contoh Encoding: ASCII, utf, latin, dsb.</li>\n",
    "\t<li>saya membahas lebih jauh tetang encoding disini:&nbsp;<br />\n",
    "\t<a href=\"https://taufiksutanto.blogspot.co.id/2018/01/pereda-sakit-kepala-urgensi-memahami.html\" target=\"_blank\">https://taufiksutanto.blogspot.co.id/2018/01/pereda-sakit-kepala-urgensi-memahami.html</a></li>\n",
    "\t<li>Berikut adalah sebuah contoh sederhana tantangan proses encoding-decoding ketika kita hendak memproses data yang berasal dari internet atau media sosial.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kita bisa menggunakan modul unidecode untuk mendapatkan representasi ASCII terdekat\n",
    "from unidecode import unidecode\n",
    "\n",
    "T = \"ḊḕḀṙ ₲ØĐ, p̾l̾e̾a̾s̾e ḧḕḶṖ ṁḕ\"\n",
    "print(unidecode(T).lower())\n",
    "# Bahasa Indonesia dan Inggris secara umum mampu direpresentasikan dalam encoding ASCII: \n",
    "# https://en.wikipedia.org/wiki/ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kita juga bisa membersihkan posting media sosial/website dengan entitas html menggunakan fungsi \"unescape\" di modul \"html\"\n",
    "from html import unescape\n",
    "\n",
    "print(unescape('Satu &lt; Tiga&nbsp;&amp; &#169; adalah simbol Copyright'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Latihan 1</strong>:</p>\n",
    "\n",
    "<p>Diberikan&nbsp;tweet berikut:<br />\n",
    "<strong>tweet&nbsp;</strong>=&nbsp; &quot;<em><strong>The #OctopiPower is &amp;gt; Sharks! &amp;amp; they&#39;re awsm! So happy to see them here <a href=\"http://www.octopusVSshark.com\" target=\"_blank\">http://www.octopusVSshark.com</a> !</strong></em>&quot;</p>\n",
    "\n",
    "<p>preprocess tweet diatas sehingga didapatkan tweet seperti ini (Gunakan sembarang modul yang mendukung):<br />\n",
    "<strong>tweet</strong>= &quot;<em><strong>the octopus power is &gt; shark ! &amp; they are awesome ! so happy to see them here !</strong></em>&quot;</p>\n",
    "\n",
    "<p><u><strong>Petunjuk/Hints</strong></u>:</p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Buat satu atau lebih fungsi untuk memudahkan, misal fungsi <em>fixTags </em>dan <em>cleanText</em>.</li>\n",
    "\t<li>fix kata &quot;<em>they&#39;re</em>&quot; dan <em>awsm </em>dengan teknik sederhana <em>dictionary fix</em>.</li>\n",
    "\t<li>Hati-hati terhadap <u><strong>urutan </strong></u>aksi di preprocessing karena akan mengakibatkan hasil yang berbeda.<br />\n",
    "\tKelak di segmen berikutnya urutan aksi ini akan disebut sebagai &quot;<big><strong>Pipelining</strong></big>&quot;.</li>\n",
    "\t<li>Code solusi latihan ini dengan dasar fikiran bahwa solusinya nanti akan bisa digunakan untuk sembarang preprocessing.</li>\n",
    "\t<li>At this point, jangan hawatirkan dulu scalability/efisiensi.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Coba jawaban Latihan [1] di cell ini: \n",
    "# Salah satu contoh jawaban latihan ini ada di cell paling bawah. \n",
    "# Namun coba untuk tidak melihat jawaban tersebut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Latihan 2</strong>:</p>\n",
    "\n",
    "<p>Bagaimana caranya memfilter kata-kata (token) yang terdiri dari huruf dan angka (misal <em>b29nf, _24x_,&nbsp;</em>dsb)?</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Coba jawaban Latihan [2] di cell ini: \n",
    "# Salah satu contoh jawaban latihan ini ada di cell paling bawah. \n",
    "# Namun coba untuk tidak melihat jawaban tersebut\n",
    "T = 'pesawat b29 dan mig276 adalah kepunyaan fhg347x dan _24x_'\n",
    "# Harapan jawaban T = 'pesawat dan adalah kepunyaan dan'\n",
    "# Petunjuk: gunakan property \"isalpha\" pada variabel string di Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Analytics</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Tidak seperti data terstruktur, data tidak terstruktur seperti teks termasuk salah satu data yang cukup sulit untuk divisualisasikan.<br />\n",
    "\t<img alt=\"\" src=\"images/11_charts.jpg\" style=\"height:150px; width:276px\" /></li>\n",
    "\t<li>Namun terdapat Tools seperti Voyant yang dapat membantu dalam visualisasi sekaligus analisis.<br />\n",
    "\t<img alt=\"\" src=\"images/11_voyant.png\" style=\"height:118px; width:426px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Voyant-dapat-digunakan-dalam-2-cara:\">Voyant dapat digunakan dalam 2 cara:</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Online</strong>:&nbsp;<a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a><br />\n",
    "\t<u>Kelebihan</u>: Sederhana &amp; portable, tanpa harus install di komputer kita.<br />\n",
    "\t<u>Kekurangan</u>: butuh koneksi internet, tidak cocok untuk data teks yang besar, privacy.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Offline </strong>di komputer kita [Java Based]</p>\n",
    "\t</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Instalasi-Voyant:\">Instalasi Voyant:</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Unduh Voyant dari &nbsp;<a href=\"https://github.com/sgsinclair/VoyantServer\" target=\"_blank\">https://github.com/sgsinclair/VoyantServer</a></li>\n",
    "\t<li>Extract Voyant ke sembarang folder (disarankan <strong>C:\\VoyantServer</strong>)</li>\n",
    "\t<li>Unduh Java JDK dari &nbsp;<a href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html&amp;nbsp\" target=\"_blank\">http://www.oracle.com/technetwork/java/javase/downloads/index.html&amp;nbsp</a>;</li>\n",
    "\t<li>Install Java</li>\n",
    "\t<li>Tambahkan variable <strong>Java Home</strong> ke &quot;System Variable&quot;</li>\n",
    "\t<li>Jalankan VoyantServer.jar dari&nbsp;C:\\VoyantServer<br />\n",
    "\t<strong>Tips</strong>: Jalankan Voyant Server sebelum Jupyter atau ganti &quot;port&quot; di VVoyant Server.</li>\n",
    "</ol>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/JavaPath.PNG\" style=\"width: 275px ; height: 300px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kita akan menggunakan data Tweet diatas ( variable **\"D\"**)\n",
    "\n",
    "## Namun sebelumnya preprocessing dilakukan pada data Text (tweet) tersebut\n",
    "## Menggunakan teknik preprocessing yang dibahas sebelumnya (dijadikan dalam suatu fungsi \"cleanText\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2 = tau.loadTweets(file='data/Tweets.json')\n",
    "D = [t['full_text'] for t in T2] # Tweet hasil crawling\n",
    "D[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopId, lemmaId = tau.LoadStopWords(lang='id') \n",
    "slangFixId = tau.loadCorpus(file = 'data/slang.dic', sep=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, d in tqdm(enumerate(D)):\n",
    "    doc = tau.cleanText(d, fix=slangFixId, lemma=lemmaId, stops = stopId, symbols_remove = True, min_charLen = 3, max_charLen = 12, fixTag= True, fixMix=True)\n",
    "    data.append(doc)\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pertama-tama kita perlu menyimpan tweet yang telah dilakukan preprocessing diatas sebagai file teks biasa\n",
    "# Menggunakan cara yang dibahas di hari pertama\n",
    "file = 'data/dataTweet.txt'\n",
    "with open(file, 'w') as f:\n",
    "    for d in data:\n",
    "        f.write(d+'\\n')\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[2]. Jalankan Voyant secara offline atau online di URL&nbsp;<a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a></p>\n",
    "\n",
    "<p>[3]. Upload file yang baru saja kita simpan (dataTweet_plain.txt).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-1:-WordClouds\">Penggunaan Voyant 1: WordClouds</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.</li>\n",
    "\t<li>slider terms: mengkontrol banyaknya terms yang disertakan.</li>\n",
    "\t<li><strong>Summary </strong>(statistics)</li>\n",
    "\t<li><strong>Documents </strong>==&gt; add more</li>\n",
    "\t<li><strong>Phrases </strong>(n-grams like)</li>\n",
    "\t<li><strong>Export </strong>Visualisasi (kanan atas - pertama)</li>\n",
    "\t<li><strong>Options </strong>(kanan atas ke-3): Font, size, stopwords, whitelist</li>\n",
    "\t<li>&quot;?&quot; ==&gt; More Help</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-2:-Bubbles\">Penggunaan Voyant 2: Bubbles</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>&nbsp;Klik tanda 4-kotak kecil (kanan atas ke-3)</li>\n",
    "\t<li>Pilih Visualization Tools ==&gt; Bubbles</li>\n",
    "\t<li>Option: hanya stopwords</li>\n",
    "\t<li>Export: Hanya PNG</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-3:-Word-Tree\">Penggunaan Voyant 3: Word Tree</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Klik branch untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-2:-Bubbles\">Penggunaan Voyant 4: Links</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Visualization Tools ==&gt; Links</li>\n",
    "\t<li>Klik sembarang terms untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-5:-Trends\">Penggunaan Voyant 5: Trends</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Document Tools ==&gt; Trends</li>\n",
    "\t<li>.. Butuh preprocessing ...&nbsp;</li>\n",
    "\t<li>Data harus terurut waktu</li>\n",
    "\t<li>Berikut contohnya</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Latihan :&nbsp;</strong></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Crawl twitter dengan salah satu topik yang sedang trending saat ini di Indonesia.&nbsp;</li>\n",
    "\t<li>Tentukan beberapa Important user-nya (centrality analysis)</li>\n",
    "\t<li>Bentuk graph komunitasnya.</li>\n",
    "\t<li>Analisa graph diatas di gephi.</li>\n",
    "\t<li>Analisa graphnya di Voyant Tools.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Vector-Space-Model---VSM\">Vector Space Model - VSM</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/vsm.png\" style=\"width: 300px; height: 213px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Outline Representasi Dokumen :</font>\n",
    "* Representasi Sparse (VSM): Binary, tf &amp;-/ idf, Custom tf-idf, BM25\n",
    "* Frequency filtering, n-grams, vocabulary based\n",
    "* Representasi Dense:&nbsp;Word Embedding (Word2Vec dan FastText)\n",
    "* Tensor to Matrix representation untuk model Machine Learning di Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\t<li>Data yang biasanya kita ketahui berbentuk <strong>tabular </strong>(tabel/kolom-baris/matriks/<em>array</em>/larik), data seperti ini disebut data terstruktur (<strong><em>structured data</em></strong>).</li>\n",
    "\t<li>Data terstruktur dapat disimpan dengan baik di&nbsp;<em>spreadsheet</em>&nbsp;(misal:&nbsp;<em>Excel/CSV</em>) atau basis data (<em>database</em>) relasional dan secara umum dapat digunakan langsung oleh berbagai model/<em>tools</em>&nbsp;statistik/data mining konvensional.</li>\n",
    "\t<li>Sebagian data yang lain memiliki &ldquo;<em>tags</em>&rdquo; yang menjelaskan elemen semantik yang berbeda di dalamnya dan cenderung tidak memiliki skema (struktur) yang statis.</li>\n",
    "\t<li>Data seperti ini disebut data<em>&nbsp;<strong>semi-structured</strong></em>, contohnya data dalam bentuk &nbsp;<strong><a href=\"http://www.w3.org/XML/\" target=\"_blank\">XML</a></strong>.</li>\n",
    "\t<li>Apa bedanya? Apa maksudnya tidak memiliki skema yang statis? Penjelasan mudahnya bayangkan sebuah data terstruktur (tabular), namun dalam setiap baris (<em>record/instance</em>)-nya tidak memiliki jumlah variabel (peubah) yang sama.</li>\n",
    "\t<li>Tentu saja data seperti ini tidak sesuai jika disimpan dan diolah dengan&nbsp;<em>tools/software</em>&nbsp;yang mengasumsikan struktur yang statis pada setiap barisnya (misal: Excel dan SPSS).</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_tipeData.png\" style=\"height: 400px ; width: 430px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "\t<li>Data multimedia seperti teks, gambar atau video <strong>tidak dapat</strong>&nbsp;<strong>secara langsung</strong>&nbsp;dianalisa dengan model statistik/data mining.</li>\n",
    "\t<li>Sebuah proses awal&nbsp;<em>(pre-process)</em>&nbsp;harus dilakukan terlebih dahulu untuk merubah data-data tidak (semi) terstruktur tersebut menjadi bentuk yang dapat digunakan oleh model statistik/data mining konvensional.</li>\n",
    "\t<li>Terdapat berbagai macam cara mengubah data-data tidak terstruktur tersebut ke dalam bentuk yang lebih sederhana, dan ini adalah suatu bidang ilmu tersendiri yang cukup dalam. Sebagai contoh saja sebuah teks biasanya dirubah dalam bentuk vektor/<em>topics</em>&nbsp;terlebih dahulu sebelum diolah.</li>\n",
    "\t<li>Vektor data teks sendiri bermacam-macam jenisnya: ada yang berdasarkan eksistensi (<strong><em>binary</em></strong>), frekuensi dokumen (<strong>tf</strong>), frekuensi dan invers jumlah dokumennya dalam corpus (<strong><a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\">tf-idf</a></strong>), <strong>tensor</strong>, dan sebagainya.</li>\n",
    "\t<li>&nbsp;Proses perubahan ini sendiri biasanya tidak&nbsp;<em>lossless</em>, artinya terdapat cukup banyak informasi yang hilang. Maksudnya bagaimana? Sebagai contoh ketika teks direpresentasikan dalam vektor (sering disebut sebagai model <strong>bag-of-words</strong>) maka informasi urutan antar kata menghilang.&nbsp;</li>\n",
    "</ul>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_structureData.png\" style=\"height:270px; width:578px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Contoh bentuk umum representasi dokumen:</strong></p>\n",
    "\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_Bentuk umum representasi dokumen.JPG\" style=\"height: 294px ; width: 620px\" /></p>\n",
    "\n",
    "<p>Pada Model <em>n-grams</em> kolom bisa juga berupa frase.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Document-Term-Matrix-:-Vector-Space-Model---VSM\">Document-Term Matrix : Vector Space Model - VSM</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/vsm_matrix.png\" style=\"width: 500px; height: 283px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_rumus tfidf.png\" style=\"height:370px; width:367px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_tfidf logic.jpg\" style=\"height:359px; width:638px\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_variant tfidf.png\" style=\"height:334px; width:955px\" /></p>\n",
    "K = |d|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan modul SciKit untuk merubah data tidak terstruktur ke VSM\n",
    "# Scikit implementation http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VSM - \"binari\"\n",
    "binary_vectorizer = CountVectorizer(binary = True)\n",
    "binari = binary_vectorizer.fit_transform(data)\n",
    "binari.shape # ukuran VSM\n",
    "# Apakah \"max_df\" dan \"min_df\" ?\n",
    "# MySQL menggunakan max_df = 0.5 (!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse vectors/matrix\n",
    "binari[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengakses Datanya\n",
    "print(binari[0].data)\n",
    "print(binari[0].indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolom dan term\n",
    "print(str(binary_vectorizer.vocabulary_)[:93])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VSM term Frekuensi : \"tf\"\n",
    "tf_vectorizer = CountVectorizer(binary = False)\n",
    "tf = tf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tf.shape) # Sama\n",
    "print(tf[0].data) # Hanya data ini yg berubah\n",
    "print(tf[0].indices) # Letak kolomnya tetap sama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = tf_vectorizer.vocabulary_\n",
    "kata_kolom = {k:v for v,k in d.items()}\n",
    "kata_kolom[99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VSM term Frekuensi : \"tf-idf\"\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tfidf.shape) # Sama\n",
    "print(tfidf[0].data) # Hanya data ini yg berubah\n",
    "print(tfidf[0].indices) # Letak kolomnya berbeda, namun jumlah kolom dan elemennya tetap sama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom tf-idf:\n",
    "* Menurut http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "* default formula tf-idf yang digunakan sk-learn adalah:\n",
    "* $tfidf = tf * log(\\frac{N}{df+1})$ ==> Smooth IDF\n",
    "* namun kita merubahnya menjadi:\n",
    "* $tfidf = tf * log(\\frac{N}{df})$ ==> Non Smooth IDF\n",
    "* $tfidf = tf * log(\\frac{N}{df+1})$ ==> linear_tf, Smooth IDF\n",
    "* $tfidf = (1+log(tf)) * log(\\frac{N}{df})$ ==> sublinear_tf, Non Smooth IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VSM term Frekuensi : \"tf-idf\"\n",
    "tfidf_vectorizer = TfidfVectorizer(smooth_idf= False, sublinear_tf=True)\n",
    "tfidf = tfidf_vectorizer.fit_transform(data)\n",
    "print(tfidf.shape) # Sama\n",
    "print(tfidf[0].data) # Hanya data ini yg berubah\n",
    "print(tfidf[0].indices) # Letak kolomnya = tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alasan melakukan filtering berdasarkan frekuensi:\n",
    "* Intuitively filter noise \n",
    "* Curse of Dimensionality (akan dibahas kemudian)\n",
    "* Computational Complexity\n",
    "* Improving accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Filtering di VSM\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_1 = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.75, min_df=5)\n",
    "tfidf_2 = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "print(tfidf_1.shape)\n",
    "print(tfidf_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Best-Match-Formula-:-BM25\">Best-Match Formula : BM25</h2>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_bm25_simple.png\" style=\"height: 123px; width: 300px;\" /></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>di IR nilai b dan k yang optimal adalah :&nbsp;<strong> <em>b</em> = 0.75&nbsp; dan k = [1.2 - 2.0]&nbsp; &nbsp;</strong><br />\n",
    "\tref:&nbsp;<em>Christopher, D. M., Prabhakar, R., &amp; Hinrich, S. C. H. &Uuml;. T. Z. E. (2008). Introduction to information retrieval.&nbsp;An Introduction To Information Retrieval,&nbsp;151, 177.</em></li>\n",
    "\t<li>Tapi kalau untuk TextMining (clustering) nilai <strong>k optimal adalah 20, nilai b = sembarang (boleh = 0.75)</strong><br />\n",
    "\tref:&nbsp;<em>Whissell, J. S., &amp; Clarke, C. L. (2011). Improving document clustering using Okapi BM25 feature weighting.&nbsp;Information retrieval,&nbsp;14(5), 466-487.</em></li>\n",
    "\t<li><strong>avgDL </strong>adalah rata-rata panjang dokumen di seluruh dataset dan <strong>DL </strong>adalah panjang dokumen D.<br />\n",
    "\thati-hati, ini berbeda dengan &nbsp;tf-idf MySQL diatas.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variasi pembentukan matriks VSM:\n",
    "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
    "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat'\n",
    "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
    "\n",
    "D = [d1, d2, d3, d4]\n",
    "# Jika kita menggunakan cara biasa: \n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Grams VSM\n",
    "# Bermanfaat untuk menangkap frase kata, misal: \"ga banget\", \"pisang goreng\", dsb\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary based VSM\n",
    "# Bermanfaat untuk menghasilkan hasil analisa yang \"bersih\"\n",
    "# variasi 2\n",
    "d1 = '@udin76, Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget sadiezz'\n",
    "d3 =  'Sudah lumayan lama bingits tukang Bakso belum lewat seru'\n",
    "d4 = 'Aduh ga banget makan Mie Ayam p4k4i kesyap, please deh'\n",
    "D = [d1,d2,d3,d4]\n",
    "Vocab = {'seru banget':0, 'seru':1, 'the best':2, 'lama':3, 'text mining':4, 'nlp':5, 'ayam':6}\n",
    "tf_vectorizer = CountVectorizer(binary = False, vocabulary=Vocab)\n",
    "tf = tf_vectorizer.fit_transform(D)\n",
    "print(tf.toarray())\n",
    "tf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab = {'seru banget':0, 'the best':1, 'lama':2, 'text mining':3, 'nlp':4, 'ayam':5}\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, lowercase=True, vocabulary=Vocab)\n",
    "vsm = tfidf_vectorizer.fit_transform(D)\n",
    "print(tfidf_vectorizer.vocabulary_)\n",
    "# VSM terurut sesuai definisi dan terkesan lebih \"bersih\"\n",
    "# Perusahaan besar biasanya memiliki menggunakan teknik ini dengan vocabulary yang comprehensif\n",
    "# Sangat cocok untuk Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><strong>Word Embeddings</strong></h2>\n",
    "\n",
    "<h2><img alt=\"\" src=\"images/3_word_embeddings.png\" style=\"height: 296px ; width: 602px\" /></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_word2vec_example.png\" style=\"height:400px; width:667px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Word2Vec</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/3_word2Vec.png\" style=\"height:400px; width:636px\" /><br />\n",
    "Dikembangkan oleh Tomas Mikolov - Google :</p>\n",
    "\n",
    "<p>Goldberg, Yoav; Levy, Omer. &quot;word2vec Explained: Deriving Mikolov et al.&#39;s Negative-Sampling Word-Embedding Method&quot;.&nbsp;<a href=\"https://en.wikipedia.org/wiki/ArXiv\">arXiv</a>:<a href=\"https://arxiv.org/abs/1402.3722\">1402.3722</a> </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/BoW_VS_WordEmbedding.png\" style=\"width: 248px; height: 372px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rubah bentuk data seperti yang dibutuhkan genSim\n",
    "# Bisa juga dilakukan dengan memodifikasi fungsi \"cleanText\" (agar lebih efisien)\n",
    "\n",
    "data_we = []\n",
    "for doc in data:\n",
    "    Tokens = [str(w) for w in TextBlob(doc).words]\n",
    "    data_we.append(Tokens)\n",
    "print(data_we[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/word2vec.html\n",
    "# train word2vec dengan data di atas\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "L = 300 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
    "model_wv = Word2Vec(data_we, min_count=5, size=L, window = 5, workers= -1)\n",
    "# min_count adalah jumlah kata minimal yang muncul di corpus\n",
    "# \"size\" adalah Dimensionality of the word vectors \n",
    "# (menurut beberapa literature untuk text disarankan 300-500)\n",
    "# \"window\" adalah jarak maximum urutan kata yang di pertimbangkan\n",
    "# workers = jumlah prosesor yang digunakan untuk menjalankan word2vec\n",
    "print('Done!...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# di data yang sebenarnya (i.e. besar) Gensim sering membutuhkan waktu cukup lama\n",
    "# Untungnya kita bisa menyimpan dan me-load kembali hasil perhitungan model word2vec, misal\n",
    "model_wv.save('data/model_w2v')\n",
    "model_wv = Word2Vec.load('data/model_w2v')\n",
    "print('Done!...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hati-hati, Word2vec menggunakan Matriks Dense\n",
    "\n",
    "<p>Penggunaan memory oleh Gensim kurang lebih sebagai berikut:</p>\n",
    "\n",
    "<p>Jumlah kata x &quot;size&quot; x 12 bytes</p>\n",
    "\n",
    "<p>Misal terdapat 100 000 kata unik dan menggunakan 200 layers, maka penggunaan memory =&nbsp;</p>\n",
    "\n",
    "<p>100,000x200x12 bytes = ~229MB</p>\n",
    "\n",
    "<p>Jika jumlah size semakin banyak, maka jumlah training data yang diperlukan juga semakin banyak, namun model akan semakin akurat.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat vector suatu kata\n",
    "vektor = model_wv.wv.__getitem__(['makassar'])\n",
    "print(len(vektor[0])) # Panjang vektor keseluruhan = jumlah neuron yang digunakan\n",
    "print(vektor[0][:5]) # 5 elemen pertama dari vektornya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencari kata terdekat menurut data training dan Word2Vec\n",
    "model_wv.wv.most_similar('baju')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat similarity antar kata\n",
    "print(model_wv.wv.similarity('makassar', 'kopi'))\n",
    "print(model_wv.wv.similarity('makassar', 'makassar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/3_cosine.png\" style=\"height:400px; width:683px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hati-hati Cosine adalah similarity bukan distance\n",
    "Hal ini akan mempengaruhi interpretasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error jika kata tidak ada di training data\n",
    "# beckman bukan beckmans ==> hence di Word Embedding PreProcessing harus thourough\n",
    "\n",
    "kata = 'makasaru'\n",
    "try:\n",
    "    print(model_wv.wv.most_similar(kata))\n",
    "except:\n",
    "    print('error! kata \"',kata,'\" tidak ada di training data')\n",
    "# ini salah satu kelemahan Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips:\n",
    "\n",
    "<p>Hati-hati GenSim tidak menggunakan seluruh kata di training data!.</p>\n",
    "\n",
    "<p>Perintah berikut akan menghasilkan kata-kata yang terdapat di vocabulary GenSim</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary = model_wv.wv.vocab\n",
    "print(str(Vocabulary.keys())[:130])\n",
    "# Gunakan vocabulary ini (rubah ke \"set\") untuk membuat program menjadi lebih robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hati-hati menginterpretasikan hasil Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\" FastText-(Facebook-2016)\">&nbsp;FastText (Facebook-2016)</h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>Menggunakan Sub-words: app, ppl, ple - apple</li>\n",
    "\t<li>Paper:&nbsp;https://arxiv.org/abs/1607.04606&nbsp;&nbsp;</li>\n",
    "\t<li>Website:&nbsp;https://fasttext.cc/</li>\n",
    "\t<li>Source:&nbsp;https://github.com/facebookresearch/fastText&nbsp;</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution penggunaan memory besar, bila timbul \"Memory Error\" kecilkan nilai L\n",
    "from gensim.models import FastText\n",
    "\n",
    "L = 100 # Jumlah neurons = ukuran vektor = jumlah kolom\n",
    "model_FT = FastText(data_we, size=L, window=4, min_count=1, workers=1)\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencari kata terdekat menurut data training dan Word2Vec\n",
    "model_FT.wv.most_similar('makassar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat similarity antar kata\n",
    "print(model_FT.wv.similarity('makassar', 'kopi'))\n",
    "print(model_FT.wv.similarity('makassar', 'makassar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec VS FastText\n",
    "try:\n",
    "    print(model_wv.wv.most_similar('makasaru'))\n",
    "except:\n",
    "    print('Word2Vec error!')\n",
    "    \n",
    "try:\n",
    "    print(model_FT.wv.most_similar('makasaru'))\n",
    "except:\n",
    "    print('FastText error!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = data_we[0]\n",
    "\n",
    "for kata in dd:\n",
    "    print(model_wv.wv.__getitem__([kata]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diskusi:\n",
    "<ul>\n",
    "\t<li>Apakah kelebihan dan kekurangan WE secara umum?</li>\n",
    "\t<li>Apakah kira-kira aplikasi WE?</li>\n",
    "\t<li>Apakah bisa dijadikan representasi dokumen? Bagaimana caranya?</li>\n",
    "\t<li>Bergantung pada apa sajakah performa model WE?</li>\n",
    "</ul>\n",
    "\n",
    "* Preprocessing apa yang sebaiknya dilakukan pada model Word Embedding?\n",
    "* Apakah Pos Tag bermanfaat disini? Jika iya bagaimana menggunakannya?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>End of Module 07</h1>\n",
    "<hr />\n",
    "<p><img alt=\"\" src=\"images/2_Studying_Linguistic.png\" style=\"height:500px; width:667px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "from nltk import word_tokenize as wt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def fixTags(T):\n",
    "    getHashtags = re.compile(r\"#(\\w+)\")\n",
    "    pisahtags = re.compile(r'[A-Z][^A-Z]*')\n",
    "    t = T\n",
    "    tagS = re.findall(getHashtags, T)\n",
    "    for tag in tagS:\n",
    "        proper_words = ' '.join(re.findall(pisahtags, tag))\n",
    "        t = t.replace('#'+tag,proper_words)\n",
    "    return t\n",
    "\n",
    "def cleanText(T, fix, patternS):\n",
    "    t = T\n",
    "    for pattern in patternS: #in case in the future we need more pattern cleaning\n",
    "        t = re.sub(pattern,' ',t)\n",
    "    t = unescape(t)\n",
    "    t = fixTags(t)\n",
    "    t = t.lower()\n",
    "    for slang, formal in fix.items():\n",
    "        t = t.replace(slang,formal)\n",
    "    t = wt(t) # tokenize\n",
    "    t = [lemmatizer.lemmatize(token) for token in t]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix = {'awsm':'awesome', \"they're\":'they are'}\n",
    "urlPattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "patternS = [urlPattern]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Jawaban Latihan No 1\n",
    "tweet = \"The #OctopiPower is &gt; Sharks! &amp; they're awsm! So happy to see them here http://www.octopusVSshark.com !\"\n",
    "T = [tweet, 'status lain']\n",
    "\n",
    "C = []\n",
    "for t in T:\n",
    "    C.append( cleanText(t, fix, patternS) )\n",
    "print(' '.join(C[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "# Contoh Jawaban Sederhana Latihan No 2\n",
    "from nltk import word_tokenize as wt\n",
    "\n",
    "T = 'pesawat b29 dan mig276 adalah kepunyaan fhg347x dan _24x_'\n",
    "T_Clean = []\n",
    "for t in wt(T):\n",
    "    if sum(c.isalpha() for c in t)==len(t):\n",
    "        T_Clean.append(t)\n",
    "print(' '.join(T_Clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> End of Module 07\n",
    "\n",
    "<hr />\n",
    "<p><img alt=\"\" src=\"images/9_Sentiment_Analysis_Meme.jpg\" /></p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
