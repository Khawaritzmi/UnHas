{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# http://tau-data.id/unhas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img alt=\"\" src=\"images/0_Cover.jpg\"/></center> \n",
    "\n",
    "## <center><font color=\"blue\">Modul 03: Data Gathering & Text Analytics</font></center>\n",
    "<b><center>(C) Taufik Sutanto - 2019</center>\n",
    "<center>tau-data Indonesia ~ https://tau-data.id ~ taufik@tau-data.id</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center><font color=\"blue\"> Data Gathering & Text Analytics</font></center>\n",
    "<img alt=\"\" src=\"images/PDS_logo.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <font color=\"blue\">Workshop Schedule</font>\n",
    "\n",
    "<center><img alt=\"\" src=\"images/Outline.jpeg\"/></center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Modules for Google Colab\n",
    "!wget https://raw.githubusercontent.com/taufikedys/UnHas/master/taudata.py\n",
    "!mkdir data\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UnHas/master/data/Tweets.json\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UnHas/master/data/dataTweet.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UnHas/master/data/kataNegID.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UnHas/master/data/kataPosID.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UnHas/master/data/slang.dic\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UnHas/master/data/stopwords_id.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UnHas/master/data/stopwords_en.txt\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UnHas/master/data/all_indo_man_tag_corpus_model.crf.tagger\n",
    "!wget -P data/ https://raw.githubusercontent.com/taufikedys/UnHas/master/data/kata_dasar.txt\n",
    "!pip install unidecode\n",
    "!pip install pyLDAvis\n",
    "!pip install textblob\n",
    "!pip install sastrawi\n",
    "!pip install twython\n",
    "!pip install tweepy\n",
    "!pip install spacy\n",
    "!pip install python-crfsuite\n",
    "!python -m spacy download en\n",
    "!python -m spacy download xx\n",
    "!python -m spacy download en_core_web_sm\n",
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import taudata as tau, seaborn as sns; sns.set()\n",
    "import tweepy, json, nltk, urllib.request\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from twython import TwythonStreamer  \n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Gathering: Crawling, Streaming, Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Analytics (SMA)\n",
    "\n",
    "<h3 id=\"SMA-adalah-sebuah-proses-pengumpulan-data-dari-media-sosial-dan-analisanya-untuk-mendapatkan-'insights'-atau-informasi-berharga-untuk-suatu-tujuan-tertentu-(definisi-adopted-dari-Gartner*)\">SMA adalah sebuah proses pengumpulan data dari media sosial dan analisanya untuk mendapatkan &#39;insights&#39; atau informasi berharga untuk suatu tujuan tertentu (definisi diadopsi dari Gartner*).</h3>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/8_SMA.JPG\" style=\"width: 600px; height: 304px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/8_SMA_Cycle.JPG\" style=\"height:300px; width:705px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/8_SMA_Techniques.JPG\" style=\"height:400px; width:574px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Crawling-Data\">Crawling/Scrapping Data</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Digital_Media_Crawling_.png\" /></p>\n",
    "\n",
    "* Credits, image source: https://www.promptcloud.com/blog/scraping-social-media-data-for-sentiment-analysis/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Social-Media-Analytics-Challenges\"><u>Tantangan Social Media Analytics</u></h3>\n",
    "\n",
    "<ul>\n",
    "\t<li>\n",
    "\t<p>Pendek (<strong>Short </strong>in lengths): bahkan terkadang tidak mengandung sebuah kalimat yang utuh menurut tata bahasa (grammar).</p>\n",
    "\t</li>\n",
    "\t<li><strong>Noise&nbsp;</strong>: Data media sosial penuh dengan noise seperti typos (salah ketik), encoding yang tidak jamak, slang, dsb.</li>\n",
    "\t<li><strong>Temporal&nbsp;</strong>: Informasi yang sedang trending biasanya hanya sesaat,<br />\n",
    "\tsehingga SMA diharapkan dilakukan dengan cepat menggunakan model-model/teknik-teknik analisa data yang efisien.</li>\n",
    "\t<li><strong>High-dimensional</strong> : Data di Media Sosial (Teks, Gambar, Video, Suara, dsb) adalah data tidak terstruktur berdimensi tinggi.</li>\n",
    "\t<li><strong>Fine-grained</strong> : Data di media sosial berasal dari banyak user yang masing-masingnya bisa jadi membahas beberapa topik yang berbeda.<br />\n",
    "\tSehingga komunitas (kelompok), topik, maupun klasifikasi yang ada menjadi besar (fine-grained).</li>\n",
    "\t<li><strong>Large in volume</strong>&nbsp;&amp; <strong>High velocity</strong>:&nbsp; Data yang sangat besar dan bertambah besar dengan cepat.</li>\n",
    "\t<li><strong>A lot of external Information</strong> : Informasi terkadang lebih banyak terkandung dari luar (eksternal) seperti url website, video, atau hal lain yang dibagikan oleh pengguna media sosial.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Case-Study:-twitter\">Case Study: twitter</h2>\n",
    "<ol>\n",
    "\t<li>API Keys</li>\n",
    "\t<li>Rules</li>\n",
    "\t<li>Crawling by searching</li>\n",
    "    <li>tweet Json</li>\n",
    "\t<li>Crawling by Streaming and Scrapping</li>\n",
    "    <li>twitter Social Media Analytics</li>\n",
    "</ol>\n",
    "<img alt=\"\" src=\"images/6_twitter.png\" style=\"width: 300px; height: 300px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>twitter API Keys</h2>\n",
    "\n",
    "<h2><img alt=\"\" src=\"images/6_Creating_API_Keys.png\" style=\"width: 854px ; height: 444px\" /></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Aturan-twitter\">Aturan, bentuk data, &amp; error codes twitter</h2>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/rest/public/rate-limiting\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/rest/public/rate-limiting\" target=\"_blank\">dev.twitter.com/rest/public/rate-limiting</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/terms/agreement-and-policy\" target=\"_blank\">https://dev.twitter.com/overview/terms/agreement-and-policy</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/api/response-codes\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/overview/api/response-codes\" target=\"_blank\">dev.twitter.com/overview/api/response-codes</a></p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><a href=\"https://dev.twitter.com/overview/api/tweets\" target=\"_blank\">https://</a><a href=\"https://dev.twitter.com/overview/api/tweets\" target=\"_blank\">dev.twitter.com/overview/api/tweets</a></p>\n",
    "\t</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh API Keys (Sesuaikan dengan API keys masing-masing)\n",
    "Ck = '' # consumer_key\n",
    "Cs = '' # consumer_secret\n",
    "At = '' # access_token\n",
    "As = '' # access_secret\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Koneksi ke twitter\n",
    "twitter = tau.twitter_connect(Ck, Cs, At, As)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'makassar'\n",
    "n = 10000 # jumlah tweet yang ingin diambil\n",
    "Tweets = tau.getTweets(twitter, topic, N = n, lan='id') # lan = 'id' atau 'en' ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bentuk tweet = Json. Contoh tweet pertama:\n",
    "Tweets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengakses data spesifik pada tweet yang pertama:\n",
    "print('tweet pertama oleh \"{}\" : \"{}\"'.format(Tweets[0]['user']['screen_name'],Tweets[0]['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan hasil crawling twitter\n",
    "fileName = 'data/Tweets.json'\n",
    "tau.saveTweets(Tweets,file=fileName)\n",
    "print('Saved to '+fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Me-load kembali jika (misal) analisa ingin dilakukan di lain waktu\n",
    "# Sengaja nama variabelnya saya bedakan (T2)\n",
    "T2 = tau.loadTweets(file='data/Tweets.json')\n",
    "print('tweet pertama oleh \"{}\" : \"{}\"'.format(T2[0]['user']['screen_name'],T2[0]['full_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengambil hanya data tweet\n",
    "D = [t['full_text'] for t in T2]\n",
    "D[:5] # 5 tweet pertama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Pemilihan-KeyWords\">Pemilihan KeyWords yang baik</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li><a href=\"https://medium.com/lingvo-masino/how-to-choose-keywords-in-twitter-9c3b85c50290\" target=\"_blank\">https://medium.com/lingvo-masino/how-to-choose-keywords-in-twitter-9c3b85c50290</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Operator (penting)\n",
    "\n",
    "<ul>\n",
    "\t<li><img alt=\"\" src=\"images/query_Operator.png\" style=\"width: 661px; height: 554px;\" /></li>\n",
    "    <li>Detail: <a href=\"https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators.html\" target=\"_blank\">https://developer.twitter.com/en/docs/tweets/search/guides/standard-operators.html</a></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari kita coba #1\n",
    "twitter = tau.twitter_connect(Ck, Cs, At, As)\n",
    "topic = 'from:jokowi'\n",
    "n = 30 # Contoh jumlah tweet yang ingin diambil\n",
    "T = tau.getTweets(twitter, topic, N = n) # lan = 'id' atau 'en' ...\n",
    "[t['full_text'] for t in T]\n",
    "\n",
    "# Lesson ... becarefull with duplicates ==> use Hash Function!!!... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Go to: https://www.latlong.net/convert-address-to-lat-long.html\n",
    "\n",
    "## Get Latitude dan Longitude dari alamat yang diinginkan\n",
    "\n",
    "**Catt**: Proses ini bisa juga digantikan dengan Google Maps API (gratis 40.000 query/bulan) dan bisa dijalankan langsung dalam program Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mari kita coba #3 gunakan google (map) untuk koordinat suatu lokasi\n",
    "# http://thoughtfaucet.com/search-twitter-by-location/\n",
    "# misal search tweet tentang \"makanan\" di Depok dan sekitarnya\n",
    "\n",
    "auth = tweepy.auth.OAuthHandler(Ck, Cs)\n",
    "auth.set_access_token(At, As)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "Geo, N = \"-5.140370,119.480620,30km\", 30\n",
    "qry = '\"coto kuda\"'\n",
    "for tweet in tweepy.Cursor(api.search,q=qry,count=100,geocode=Geo).items(N):\n",
    "    print([tweet.created_at, tweet.text.encode('utf-8'), tweet.user.id, tweet.geo])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Streaming-Data\">Streaming and Scrapping Data</h1>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/Meme_Streaming_Data.jpg\" style=\"width: 307px; height: 309px;\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming tweets. Untuk percobaan pilih topicS sesuatu yg sedang trending/populer \"saat ini\".\n",
    "# Atau bisa coba dengan mengirim tweet sendiri :)\n",
    "def streamTwitter(topicS, lang):\n",
    "    class MyStreamer(TwythonStreamer):\n",
    "        def on_success(self, data):\n",
    "            global count\n",
    "            count+=1\n",
    "            print('tweet from {}, post: {}'.format(data['user']['screen_name'], data['text']))\n",
    "            if count==maxTweet:\n",
    "                print('\\nFinished streaming %.0f tweets' %(maxTweet)); self.disconnect()\n",
    "        def on_error(self, status_code, data):\n",
    "            print('Error Status = %s' %status_code); self.disconnect()\n",
    "\n",
    "    while count<maxTweet:\n",
    "        stream = MyStreamer(Ck, Cs, At, As)\n",
    "        stream.statuses.filter(track=topicS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweet, count = 2, 0 # Rubah sesuai dengan kebutuhan, Untuk percobaan ini cukup (misal) 3 tweet\n",
    "lang = set(['en','id']) # bahasa bisa dipilih > 1\n",
    "topicS = ['coto duren'] # Bisa>1\n",
    "\n",
    "streamTwitter(topicS, lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping (going beyond 7 days)\n",
    "\n",
    "### Solusi bagi yang belum memiliki API keys\n",
    "\n",
    "1. Buka browser **FireFox**\n",
    "2. Go to: https://twitter.com/search-advanced   (Tidak perlu login)\n",
    "3. Search sesuai keinginan/kebutuhan\n",
    "4. Save as Web Complete (misal di desktop) sebagai **data.html**\n",
    "5. Upload ke Google Colab\n",
    "6. Jalankan cell dibawah ini\n",
    "7. Unduh (download) file csv hasil scrapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh mengubah html hasil scrapping ke CSV\n",
    "fileSource = 'data.html'\n",
    "fileOutput = 'data_twitter.csv'\n",
    "\n",
    "tau.twitter_html2csv(fileSource, fileOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "URL = 'http://bps.go.id'\n",
    "Doc = urllib.request.urlopen(URL).read()\n",
    "Doc = bs(Doc,'lxml').text\n",
    "print(Doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "URL = 'http://bps.go.id'\n",
    "doc = urllib.request.urlopen(URL).read()\n",
    "doc = tau.text_from_html(doc)\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text Analytics</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Tidak seperti data terstruktur, data tidak terstruktur seperti teks termasuk salah satu data yang cukup sulit untuk divisualisasikan.<br />\n",
    "\t<img alt=\"\" src=\"images/11_charts.jpg\" style=\"height:150px; width:276px\" /></li>\n",
    "\t<li>Namun terdapat Tools seperti Voyant yang dapat membantu dalam visualisasi sekaligus analisis.<br />\n",
    "\t<img alt=\"\" src=\"images/11_voyant.png\" style=\"height:118px; width:426px\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Voyant-dapat-digunakan-dalam-2-cara:\">Voyant dapat digunakan dalam 2 cara:</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>\n",
    "\t<p><strong>Online</strong>:&nbsp;<a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a><br />\n",
    "\t<u>Kelebihan</u>: Sederhana &amp; portable, tanpa harus install di komputer kita.<br />\n",
    "\t<u>Kekurangan</u>: butuh koneksi internet, tidak cocok untuk data teks yang besar, privacy.</p>\n",
    "\t</li>\n",
    "\t<li>\n",
    "\t<p><strong>Offline </strong>di komputer kita [Java Based]</p>\n",
    "\t</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Instalasi-Voyant:\">Instalasi Voyant:</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Unduh Voyant dari &nbsp;<a href=\"https://github.com/sgsinclair/VoyantServer\" target=\"_blank\">https://github.com/sgsinclair/VoyantServer</a></li>\n",
    "\t<li>Extract Voyant ke sembarang folder (disarankan <strong>C:\\VoyantServer</strong>)</li>\n",
    "\t<li>Unduh Java JDK dari &nbsp;<a href=\"http://www.oracle.com/technetwork/java/javase/downloads/index.html&amp;nbsp\" target=\"_blank\">http://www.oracle.com/technetwork/java/javase/downloads/index.html&amp;nbsp</a>;</li>\n",
    "\t<li>Install Java</li>\n",
    "\t<li>Tambahkan variable <strong>Java Home</strong> ke &quot;System Variable&quot;</li>\n",
    "\t<li>Jalankan VoyantServer.jar dari&nbsp;C:\\VoyantServer<br />\n",
    "\t<strong>Tips</strong>: Jalankan Voyant Server sebelum Jupyter atau ganti &quot;port&quot; di VVoyant Server.</li>\n",
    "</ol>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/JavaPath.PNG\" style=\"width: 275px ; height: 300px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kita akan menggunakan data Tweet diatas ( variable **\"D\"**)\n",
    "\n",
    "## Namun sebelumnya preprocessing dilakukan pada data Text (tweet) tersebut\n",
    "## Menggunakan teknik preprocessing yang dibahas sebelumnya (dijadikan dalam suatu fungsi \"cleanText\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2 = tau.loadTweets(file='data/Tweets.json')\n",
    "D = [t['full_text'] for t in T2] # Tweet hasil crawling\n",
    "D[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopId, lemmaId = tau.LoadStopWords(lang='id') \n",
    "slangFixId = tau.loadCorpus(file = 'data/slang.dic', sep=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i, d in tqdm(enumerate(D)):\n",
    "    doc = tau.cleanText(d, fix=slangFixId, lemma=lemmaId, stops = stopId, symbols_remove = True, min_charLen = 3, max_charLen = 12, fixTag= True, fixMix=True)\n",
    "    data.append(doc)\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pertama-tama kita perlu menyimpan tweet yang telah dilakukan preprocessing diatas sebagai file teks biasa\n",
    "# Menggunakan cara yang dibahas di hari pertama\n",
    "file = 'data/dataTweet.txt'\n",
    "with open(file, 'w') as f:\n",
    "    for d in data:\n",
    "        f.write(d+'\\n')\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>[2]. Jalankan Voyant secara offline atau online di URL&nbsp;<a href=\"https://voyant-tools.org/\" target=\"_blank\">https://voyant-tools.org/</a></p>\n",
    "\n",
    "<p>[3]. Upload file yang baru saja kita simpan (dataTweet_plain.txt).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-1:-WordClouds\">Penggunaan Voyant 1: WordClouds</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.</li>\n",
    "\t<li>slider terms: mengkontrol banyaknya terms yang disertakan.</li>\n",
    "\t<li><strong>Summary </strong>(statistics)</li>\n",
    "\t<li><strong>Documents </strong>==&gt; add more</li>\n",
    "\t<li><strong>Phrases </strong>(n-grams like)</li>\n",
    "\t<li><strong>Export </strong>Visualisasi (kanan atas - pertama)</li>\n",
    "\t<li><strong>Options </strong>(kanan atas ke-3): Font, size, stopwords, whitelist</li>\n",
    "\t<li>&quot;?&quot; ==&gt; More Help</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-2:-Bubbles\">Penggunaan Voyant 2: Bubbles</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>&nbsp;Klik tanda 4-kotak kecil (kanan atas ke-3)</li>\n",
    "\t<li>Pilih Visualization Tools ==&gt; Bubbles</li>\n",
    "\t<li>Option: hanya stopwords</li>\n",
    "\t<li>Export: Hanya PNG</li>\n",
    "</ol>\n",
    "\n",
    "<p>&nbsp;</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-3:-Word-Tree\">Penggunaan Voyant 3: Word Tree</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Klik branch untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-2:-Bubbles\">Penggunaan Voyant 4: Links</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Visualization Tools ==&gt; Links</li>\n",
    "\t<li>Klik sembarang terms untuk expand</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"Penggunaan-Voyant-5:-Trends\">Penggunaan Voyant 5: Trends</h3>\n",
    "\n",
    "<ol>\n",
    "\t<li>Upload teks yang akan di analisa: hasil cluster/ suatu kategori/ topics / raw text.<br />\n",
    "\tAtau file yang sudah terupload sebelumnya</li>\n",
    "\t<li>Document Tools ==&gt; Trends</li>\n",
    "\t<li>.. Butuh preprocessing ...&nbsp;</li>\n",
    "\t<li>Data harus terurut waktu</li>\n",
    "\t<li>Berikut contohnya</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Latihan :&nbsp;</strong></p>\n",
    "\n",
    "<ol>\n",
    "\t<li>Crawl twitter dengan salah satu topik yang sedang trending saat ini di Indonesia.&nbsp;</li>\n",
    "\t<li>Tentukan beberapa Important user-nya (centrality analysis)</li>\n",
    "\t<li>Bentuk graph komunitasnya.</li>\n",
    "\t<li>Analisa graph diatas di gephi.</li>\n",
    "\t<li>Analisa graphnya di Voyant Tools.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <font color=\"blue\">Outline Sentiment Analysis :</font>\n",
    "\n",
    "* Corpus-Based Sentiment Analysis\n",
    "* Metode Supervised untuk Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/9_Sentiment_Analysis_Meme.jpg\" style=\"height:300px; width:400px\" /></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Apakah sentiment analysis?</strong></p>\n",
    "\n",
    "<p>Sentiment Analysis adalah suatu proses komputasi untuk menentukan apakah suatu penrnyataan bermakna positive, negative, atau netral.</p>\n",
    "\n",
    "<p>Terkadang disebut juga sebagai&nbsp;<strong>opinion mining.</strong></p>\n",
    "\n",
    "<p><strong>Contoh aplikasi Sentiment Analysis</strong></p>\n",
    "\n",
    "<ul>\n",
    "\t<li><strong>Business: tanggapan konsumen atas suatu produk</strong>.</li>\n",
    "\t<li><strong>Politics:&nbsp;</strong>Sentimen masyarakat sebagai strategi pemenangan pemilu/pilkada.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><img alt=\"\" src=\"images/9_SA_techniques.jpg\" style=\"height:300px; width:536px\" /></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon Based berdasarkan \n",
    "# pattern = https://www.clips.uantwerpen.be/pages/pattern-en#sentiment\n",
    "Sentence = \"I love Bakpia so much it's delicious\"\n",
    "T = TextBlob(Sentence)\n",
    "print(T.sentiment)\n",
    "print('Polarity=Sentimen =', T.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana Dengan Bahasa Indonesia?\n",
    "<p>[A simple trick]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = 'coto Kuda mantep tenan euy'\n",
    "K = TextBlob(kalimat).translate(to='en')\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SenSub_ID(kalimat):\n",
    "    K = TextBlob(kalimat).translate(to='en')\n",
    "    pol,sub = K.sentiment\n",
    "    if pol>0:\n",
    "        pol='positive'\n",
    "    elif pol<0:\n",
    "        pol='negative'\n",
    "    else:\n",
    "        pol = 'netral'\n",
    "    if sub>0.5:\n",
    "        sub = 'Subjektif'\n",
    "    else:\n",
    "        sub = \"Objektif\"\n",
    "    return pol, sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalimat = 'hari ini cerah'\n",
    "SenSub_ID(kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "# Warning, mungkin lambat karena membentuk model classifier* terlebih dahulu.\n",
    "# *Berdasarkan NLTK corpus ==> Language dependent\n",
    "Sentence = \"Textblob is amazingly simple to use\"\n",
    "blob = TextBlob(Sentence, analyzer=NaiveBayesAnalyzer())\n",
    "blob.sentiment\n",
    "# Good Explanation: https://medium.com/nlpython/sentiment-analysis-analysis-ee5da4448e37\n",
    "# Output probabilitas prediksinya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana dengan Sentiment Analysis menggunakan NBC untuk Bahasa indonesia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "def bentukClassifier(P, Ne, Nt):\n",
    "    positive_features = [(word_feats(pos), 'pos') for pos in P]\n",
    "    negative_features = [(word_feats(neg), 'neg') for neg in Ne]\n",
    "    neutral_features = [(word_feats(neu), 'neu') for neu in Nt]\n",
    "    train_set = negative_features + positive_features + neutral_features\n",
    "    return NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "def prediksiSentiment(sentence, model):\n",
    "    pos, neg = 0, 0\n",
    "    words = sentence.lower().split(' ') # sebaiknya menggunakan n-gram\n",
    "    for word in words:\n",
    "        classResult = model.classify( word_feats(word) )\n",
    "        if classResult == 'neg':\n",
    "            neg = neg + 1\n",
    "        if classResult == 'pos':\n",
    "            pos = pos + 1\n",
    "    if pos>neg:\n",
    "        return 'positif'\n",
    "    elif pos==neg:\n",
    "        return 'netral'\n",
    "    else:\n",
    "        return 'negatif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = [ 'keren', 'suka', 'cinta', 'bagus', 'mantap', 'sadis', 'top']\n",
    "emot_pos = [':)', ':D', \"<3\"]\n",
    "Ne = [ 'jelek', 'benci','buruk', 'najis']\n",
    "emot_neg = [':(', \":'(\"]\n",
    "Nt = [ 'bakso','film','pisang','pagi','makan','kopi','minum','sambil','abis']\n",
    "\n",
    "model = bentukClassifier(P, Ne, Nt) \n",
    "\n",
    "sentence = \"makan pempek minumnya teh panas mantap\"\n",
    "prediksiSentiment(sentence,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negasi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Negasi = ['tidak', 'ngga', 'engga', 'enggak', 'ndak', 'endak', 'tdk']\n",
    "Pos = P + [n +' '+ ne for n,ne in zip(Negasi, Ne)] + emot_pos\n",
    "Neg = Ne + [n +' '+ po for n,po in zip(Negasi, P)] + emot_neg\n",
    "\n",
    "print(Pos, Neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pos = [t.strip() for t in tau.LoadDocuments(file = 'data/kataPosID.txt')[0]]\n",
    "Neg = [t.strip() for t in tau.LoadDocuments(file = 'data/kataNegID.txt')[0]]\n",
    "\n",
    "print(len(Pos), len(Neg))\n",
    "print(Pos[:10],'\\n', Neg[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagaimana jika mau melakukannya dengan model klasifikasi (supervised learning) lain seperti modul sebelumnya?\n",
    "(e.g. SVM, NN, DT, k-NN, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text Classification : independent variable\n",
    "d1 = 'Minum kopi pagi-pagi sambil makan pisang goreng is the best'\n",
    "d2 = 'Belajar NLP dan Text Mining ternyata seru banget'\n",
    "d3 = 'Palembang agak mendung hari ini'\n",
    "d4 =  'Sudah lumayan lama tukang Bakso belum lewat'\n",
    "d5 = 'Aduh ga banget makan Mie Ayam pakai kecap, please deh'\n",
    "d6 = 'Benci banget kalau melihat orang buang sampah sembarangan di jalan'\n",
    "d7 = 'Kalau liat orang ga taat aturan rasanya ingin ngegampar aja'\n",
    "d8 = 'Nikmatnya meniti jalan jalan penuh romansa di tengah kota bernuansa pendidikan'\n",
    "d9 = 'kemajuan bangsa ini ada pada kegigihan masyarakat dalam belajar dan bekerja'\n",
    "D = [d1,d2,d3,d4,d5,d6,d7,d8,d9]\n",
    "'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent variable, misal 0=positif, 1=netral, 2=negatif\n",
    "Class = [0,0,1,1,2,2,2,1,0]\n",
    "dic = {0:'positif', 1:'netral', 2:'negatif'}\n",
    "print([dic[c] for c in Class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bentuk VSM-nya seperti kemarin (skip preprocessing)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "vsm = vectorizer.fit_transform(D)\n",
    "\n",
    "print(vsm.shape)\n",
    "str(vectorizer.vocabulary_)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lakukan klasifikasi (misal dengan SVM)\n",
    "dSVM = svm.SVC(kernel='linear')\n",
    "sen = dSVM.fit(vsm, Class).predict(vsm)\n",
    "print(accuracy_score(Class, sen))\n",
    "# Memakai seluruh training data karena sampel yang sangat kecil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better yet, gunakan vocabulary-based VSM dan-atau Post-Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Supplementary\">Supplementary</h2>\n",
    "\n",
    "<p>* Negasi suatu kata bukan berarti memiliki sentimen kebalikannya. Misal &quot;jelek&quot; dan &quot;tidak jelek&quot; (terrible vs not terrible).</p>\n",
    "\n",
    "<p><img alt=\"\" src=\"images/negation_sentiments.png\" /></p>\n",
    "\n",
    "<p>[*]. Zhu, X., Guo, H., Mohammad, S., &amp; Kiritchenko, S. (2014). An empirical study on the effect of negation words on sentiment. In&nbsp;<i>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</i>&nbsp;(Vol. 1, pp. 304-313).</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Makna positive/negative atau pro/kontra subjective (bias) terhadap user.\n",
    "* StopWords removal in general is a bad idea\n",
    "* learn the lingo in your topic, sentiment expressions are different across fields, languages, and regions.\n",
    "* Sarcasm perlu konteks untuk di deteksi dengan tepat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Feature-Engineering/Extraction\">Feature Engineering/Extraction</h2>\n",
    "\n",
    "<ul>\n",
    "\t<li>Ketimbang pemilihan model yang optimal, beberapa literature sudah melaporkan bahwa feature engineering/extraction lebih efektif [1].</li>\n",
    "\t<li>Selain itu, pendekatan semantic dalam FE juga lebih plausible untuk dilakukan.</li>\n",
    "\t<li>Tabel berikut adalah contoh FE yang bisa dilakukan spesifik terhadap model SA.</li>\n",
    "\t<li><img alt=\"\" src=\"images/SA_Analysis_Features.png\" style=\"width: 544px; height: 425px;\" /></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catatan penting di Sentimen Analysis:\n",
    "\n",
    "1. Hati-hati melakukan stopwords dan frequency filtering.\n",
    "2. Menggunakan Post-Tag Adjective dan-atau vocabulary-based VSM dapat membantu model sentiment analysis\n",
    "3. feature engineering lebih penting ketimbang model yang kompleks.\n",
    "4. Corpus bisa jadi harus berbeda untuk setiap konteks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latihan:\n",
    "\n",
    "1. Lakukan Sentimen Analysis pada data Tweet menggunakan himpunan Lexicon diatas.\n",
    "2. Membuat piechart proporsi sentimen\n",
    "3. Memisahkan seluruh Tweet yang pos, neg, dan netral\n",
    "4. Apakah hasilnya cukup baik? Apa yang bisa dilakukan untuk memperbaiki hasil?\n",
    "5. Export masing-masing ke text file untuk dianalisa lebih lanjut (misal text analytics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopId, lemmaId = tau.LoadStopWords(lang='id') \n",
    "slangFixId = tau.loadCorpus(file = 'data/slang.dic', sep=':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2 = tau.loadTweets(file='data/Tweets.json')\n",
    "D = [t['full_text'] for t in T2] # Tweet hasil crawling\n",
    "data = []\n",
    "for i, d in tqdm(enumerate(D)):\n",
    "    doc = tau.cleanText(d, fix=slangFixId, symbols_remove = True, min_charLen = 3, max_charLen = 12, fixTag= True, fixMix=True)\n",
    "    data.append(doc)\n",
    "print(data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jawaban di cell ini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center><font color=\"blue\"> End of Module 03\n",
    "\n",
    "<hr />\n",
    "<p><img alt=\"\" src=\"images/6_SocMed_cartoon.png\" /></p>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
